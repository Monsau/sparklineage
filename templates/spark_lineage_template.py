#!/usr/bin/env python3
"""
TEMPLATE CODE SPARK LINEAGE OPENMETADATA - VERSION MINIMALE FONCTIONNELLE
Extrait de l'impl√©mentation r√©ussie
"""

from pyspark.sql import SparkSession

# =================================================================
# CONFIGURATION SPARK POUR LINEAGE OPENMETADATA
# =================================================================

def create_spark_session_with_lineage(
    app_name="SparkLineageDemo",
    openmetadata_host="http://host.docker.internal:8585/api",
    jwt_token="YOUR_JWT_TOKEN_HERE",
    pipeline_service_name="spark_lineage_service", 
    pipeline_name="spark_lineage_pipeline"
):
    """
    Cr√©e une session Spark configur√©e pour le lineage OpenMetadata
    
    Args:
        app_name: Nom de l'application Spark
        openmetadata_host: URL de l'API OpenMetadata  
        jwt_token: Token JWT pour l'authentification
        pipeline_service_name: Nom du service pipeline dans OpenMetadata
        pipeline_name: Nom du pipeline dans OpenMetadata
    
    Returns:
        SparkSession configur√©e pour le lineage
    """
    
    spark = (
        SparkSession.builder
        .master("local")
        .appName(app_name)
        
        # ‚úÖ CRITIQUE: JARs obligatoires
        .config(
            "spark.jars",
            "/opt/bitnami/spark/ivy/openmetadata-spark-agent.jar,"
            "/opt/bitnami/spark/ivy/mysql-connector-j-8.0.33.jar"
        )
        
        # ‚úÖ CRITIQUE: Listener OpenLineage pour capturer les √©v√©nements
        .config(
            "spark.extraListeners",
            "io.openlineage.spark.agent.OpenLineageSparkListener"
        )
        
        # ‚úÖ CRITIQUE: Transport OpenMetadata
        .config("spark.openmetadata.transport.type", "openmetadata")
        .config("spark.openmetadata.transport.hostPort", openmetadata_host)
        .config("spark.openmetadata.transport.jwtToken", jwt_token)
        
        # ‚úÖ CRITIQUE: Configuration du pipeline
        .config("spark.openmetadata.transport.pipelineServiceName", pipeline_service_name)
        .config("spark.openmetadata.transport.pipelineName", pipeline_name)
        
        # ‚úÖ Optionnel: Configuration additionnelle
        .config("spark.openmetadata.transport.timeout", "30")
        .config(
            "spark.openmetadata.transport.databaseServiceNames",
            "mysql-source-service,mysql-target-service"
        )
        
        .getOrCreate()
    )
    
    return spark

# =================================================================
# FONCTIONS ETL G√âN√âRATEURS DE LINEAGE
# =================================================================

def read_mysql_table(spark, host, port, database, table, user="root", password="password"):
    """
    Lit une table MySQL - G√âN√àRE AUTOMATIQUEMENT LE LINEAGE INPUT
    
    Args:
        spark: Session Spark
        host: Host MySQL
        port: Port MySQL  
        database: Nom de la base
        table: Nom de la table
        user: Utilisateur MySQL
        password: Mot de passe MySQL
    
    Returns:
        DataFrame Spark
    """
    
    df = (
        spark.read.format("jdbc")
        .option("url", f"jdbc:mysql://{host}:{port}/{database}")
        .option("dbtable", table)
        .option("user", user)
        .option("password", password)
        .option("driver", "com.mysql.cj.jdbc.Driver")
        .load()
    )
    
    print(f"‚úÖ Table lue: {host}:{port}/{database}.{table}")
    return df

def write_mysql_table(df, host, port, database, table, mode="overwrite", user="root", password="password"):
    """
    √âcrit une table MySQL - G√âN√àRE AUTOMATIQUEMENT LE LINEAGE OUTPUT
    
    Args:
        df: DataFrame √† √©crire
        host: Host MySQL
        port: Port MySQL
        database: Nom de la base
        table: Nom de la table
        mode: Mode d'√©criture (overwrite, append, etc.)
        user: Utilisateur MySQL
        password: Mot de passe MySQL
    """
    
    (
        df.write.format("jdbc")
        .option("url", f"jdbc:mysql://{host}:{port}/{database}")
        .option("dbtable", table)
        .option("user", user)
        .option("password", password)
        .option("driver", "com.mysql.cj.jdbc.Driver")
        .mode(mode)
        .save()
    )
    
    print(f"‚úÖ Table √©crite: {host}:{port}/{database}.{table}")

# =================================================================
# PIPELINE ETL COMPLET AVEC LINEAGE
# =================================================================

def run_etl_pipeline_with_lineage():
    """
    Pipeline ETL complet qui g√©n√®re automatiquement le lineage
    """
    
    print("=" * 60)
    print("üöÄ D√©marrage Pipeline ETL avec Lineage OpenMetadata")
    print("=" * 60)
    
    # ‚úÖ √âtape 1: Cr√©er session Spark avec lineage
    spark = create_spark_session_with_lineage(
        app_name="ETLLineageDemo",
        openmetadata_host="http://host.docker.internal:8585/api",
        jwt_token="YOUR_ACTUAL_JWT_TOKEN",  # ‚ö†Ô∏è REMPLACER PAR VOTRE TOKEN
        pipeline_service_name="etl_lineage_service",
        pipeline_name="customers_etl_pipeline"
    )
    
    try:
        # ‚úÖ √âtape 2: Lecture source (g√©n√®re INPUT lineage)
        print("\nüì• Lecture des donn√©es source...")
        source_df = read_mysql_table(
            spark=spark,
            host="mysql-source",
            port=3306,
            database="source_db", 
            table="customers"
        )
        
        print(f"   Nombre d'enregistrements: {source_df.count()}")
        
        # ‚úÖ √âtape 3: Transformations (g√©n√®re PROCESSING lineage)
        print("\nüîÑ Application des transformations...")
        
        # Exemple de transformations qui seront track√©es
        transformed_df = (
            source_df
            .filter(source_df.country.isNotNull())  # Filtrage
            .select("customer_id", "customer_name", "email", "city", "country", "created_at")  # S√©lection
            .withColumn("processed_at", spark.sql("SELECT current_timestamp()").collect()[0][0])  # Ajout colonne
        )
        
        print("   Transformations appliqu√©es: filtrage + s√©lection + ajout colonne")
        
        # ‚úÖ √âtape 4: √âcriture cible (g√©n√®re OUTPUT lineage)
        print("\nüíæ √âcriture vers la cible...")
        write_mysql_table(
            df=transformed_df,
            host="mysql-target",
            port=3306,
            database="target_db",
            table="customers_copy",
            mode="overwrite"
        )
        
        # ‚úÖ √âtape 5: V√©rification
        print("\nüîç V√©rification des donn√©es √©crites...")
        verification_df = read_mysql_table(
            spark=spark,
            host="mysql-target", 
            port=3306,
            database="target_db",
            table="customers_copy"
        )
        
        print(f"   Nombre d'enregistrements √©crits: {verification_df.count()}")
        
        print("\n‚úÖ Pipeline ETL termin√© avec succ√®s!")
        print("üîó Le lineage a √©t√© automatiquement g√©n√©r√© par l'agent OpenMetadata")
        
    except Exception as e:
        print(f"‚ùå Erreur dans le pipeline: {e}")
        
    finally:
        print("\nüîÑ Arr√™t de la session Spark...")
        spark.stop()

# =================================================================
# CONFIGURATION ALTERNATIVE: TRANSPORT HTTP
# =================================================================

def create_spark_session_http_transport(
    app_name="SparkLineageHTTP",
    openmetadata_api_url="http://host.docker.internal:8585/api/v1/lineage",
    jwt_token="YOUR_JWT_TOKEN_HERE"
):
    """
    Version alternative avec transport HTTP direct
    """
    
    spark = (
        SparkSession.builder
        .master("local")
        .appName(app_name)
        .config("spark.jars", "/path/to/jars/openmetadata-spark-agent.jar,/path/to/jars/mysql-connector.jar")
        .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener")
        
        # Transport HTTP au lieu d'OpenMetadata
        .config("spark.openlineage.transport.type", "http")
        .config("spark.openlineage.transport.url", openmetadata_api_url)
        .config("spark.openlineage.transport.auth.type", "api_key")
        .config("spark.openlineage.transport.auth.apiKey", jwt_token)
        .config("spark.openlineage.namespace", "spark_demo")
        
        .getOrCreate()
    )
    
    return spark

# =================================================================
# SCRIPT PRINCIPAL
# =================================================================

if __name__ == "__main__":
    print("TEMPLATE CODE SPARK LINEAGE OPENMETADATA")
    print("=" * 50)
    print("Ce template contient:")
    print("1. ‚úÖ Configuration Spark pour lineage OpenMetadata")
    print("2. ‚úÖ Fonctions ETL g√©n√©rateurs de lineage")  
    print("3. ‚úÖ Pipeline complet avec lecture ‚Üí transformation ‚Üí √©criture")
    print("4. ‚úÖ Configuration alternative transport HTTP")
    print("\nPour utiliser:")
    print("1. Remplacez YOUR_ACTUAL_JWT_TOKEN par votre token")
    print("2. Adaptez les hosts/ports √† votre environnement")
    print("3. Ex√©cutez: run_etl_pipeline_with_lineage()")
    
    # D√©commentez pour ex√©cuter:
    # run_etl_pipeline_with_lineage()