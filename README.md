# ---
# English Translation

## Spark + OpenMetadata | Automatic Data Lineage

This project provides an integration solution for automatic data lineage tracking in Apache Spark, with direct exposure in OpenMetadata. It is intended for technical teams wishing to industrialize the traceability of their Spark pipelines without modifying application code.

...existing code...

# ---
# Traducci√≥n al Espa√±ol

## Spark + OpenMetadata | Linaje de Datos Autom√°tico

Este proyecto proporciona una soluci√≥n de integraci√≥n para el seguimiento autom√°tico del linaje de datos en Apache Spark, con exposici√≥n directa en OpenMetadata. Est√° destinado a equipos t√©cnicos que deseen industrializar la trazabilidad de sus pipelines de Spark sin modificar el c√≥digo de la aplicaci√≥n.

...existing code...

# ---
# ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿ•ŸÑŸâ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©

## ÿ≥ÿ®ÿßÿ±ŸÉ + ÿ£Ÿàÿ®ŸÜ ŸÖŸäÿ™ÿßÿØÿßÿ™ÿß | ÿ™ÿ™ÿ®ÿπ ŸÜÿ≥ÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ŸÑŸÇÿßÿ¶Ÿä

ŸäŸàŸÅÿ± Ÿáÿ∞ÿß ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ ÿ≠ŸÑÿßŸã ŸÑŸÑÿ™ŸÉÿßŸÖŸÑ ŸÑÿ™ÿ™ÿ®ÿπ ŸÜÿ≥ÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ™ŸÑŸÇÿßÿ¶ŸäŸãÿß ŸÅŸä Apache Sparkÿå ŸÖÿπ ÿπÿ±ÿ∂ ŸÖÿ®ÿßÿ¥ÿ± ŸÅŸä OpenMetadata. ŸàŸáŸà ŸÖÿÆÿµÿµ ŸÑŸÑŸÅÿ±ŸÇ ÿßŸÑŸÅŸÜŸäÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ±ÿ∫ÿ® ŸÅŸä ÿ£ÿ™ŸÖÿ™ÿ© ÿ™ÿ™ÿ®ÿπ ÿÆÿ∑Ÿàÿ∑ ÿ£ŸÜÿßÿ®Ÿäÿ® Spark ÿßŸÑÿÆÿßÿµÿ© ÿ®ŸáŸÖ ÿØŸàŸÜ ÿ™ÿπÿØŸäŸÑ ŸÉŸàÿØ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ.

...existing code...

# Spark + OpenMetadata | Lineage Automatique de Donn√©es

Ce projet fournit une solution d'int√©gration pour le suivi automatique du lineage des donn√©es dans Apache Spark, avec exposition directe dans OpenMetadata. Il s'adresse aux √©quipes techniques souhaitant industrialiser la tra√ßabilit√© de leurs pipelines Spark, sans modification du code applicatif.


## Fonctionnalit√©s principales

```mermaid
flowchart LR
    subgraph Sources[üìä Sources de Donn√©es]
        S1[üóÑÔ∏è MySQL Source]
        S2[üìÅ Fichiers CSV] 
        S3[ü™£ S3 Bucket]
        S4[üêò PostgreSQL]
    end
    
    subgraph Spark[‚ö° Apache Spark]
        J1[üìù Job ETL]
        J2[üîÑ Transformations]
        J3[üìä Agr√©gations]
    end
    
    subgraph Targets[üéØ Destinations]
        T1[üóÑÔ∏è MySQL Target]
        T2[üìä Data Warehouse]
        T3[üìà Analytics DB]
    end
    
    subgraph OM[üìã OpenMetadata]
        L1[üîó Lineage Graph]
        L2[ÔøΩ Data Quality]
        L3[‚è±Ô∏è Pipeline History]
    end
    
    Sources --> Spark
    Spark --> Targets
    Spark -.->|Auto-track| OM
    
    style Spark fill:#ff6b35
    style OM fill:#4ecdc4
```

- D√©couverte automatique des sources et destinations
- Suivi du lineage en temps r√©el pour chaque transformation Spark
- Visualisation graphique dans OpenMetadata
- Int√©gration sans modification du code applicatif (configuration uniquement)
- M√©tadonn√©es enrichies (sch√©mas, colonnes, transformations)


## Modes d'int√©gration


### Option A : Environnement complet (Docker)

```mermaid
graph TB
    subgraph Docker[üê≥ Docker Compose]
        D1[üìä OpenMetadata]
        D2[‚ö° Spark Master]
        D3[üîß Spark Worker] 
        D4[üóÑÔ∏è MySQL Source]
        D5[üóÑÔ∏è MySQL Target]
    end
    
    D2 --> D3
    D2 -.->|Lineage| D1
    D4 --> D2
    D2 --> D5
```


### Option B : Int√©gration dans un cluster Spark existant

```mermaid
graph LR
    subgraph Existing[üè¢ Ton Infrastructure]
        E1[‚ö° Spark Cluster]
        E2[üìä Tes DBs]
        E3[üîÑ Tes Jobs ETL]
    end
    
    subgraph Add[‚ûï √Ä Ajouter]
        A1[üìã OpenMetadata]
        A2[üîß Agent JAR]
        A3[‚öôÔ∏è Config Spark]
    end
    
    Existing --> Add
    Add -.->|Lineage Auto| A1
```

---


## Option A : D√©ploiement Docker

### Installation rapide

```bash
# 1. Clone le projet
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage

# 2. Lance tout l'environnement
docker-compose up -d

# 3. Attends que tout d√©marre (30 sec environ)
docker-compose ps

# 4. Lance le job d'exemple
./run-example.sh
```

#### Lancement manuel
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/bitnami/spark/jars/openmetadata-spark-agent.jar,/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_TOKEN" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### üìä Services Disponibles

| Service | URL | Credentials |
|---------|-----|-------------|
| üìã **OpenMetadata UI** | http://localhost:8585 | admin/admin |
| ‚ö° **Spark Master** | http://localhost:8080 | - |
| üóÑÔ∏è **MySQL Source** | localhost:3308 | root/password |
| üóÑÔ∏è **MySQL Target** | localhost:3307 | root/password |

---


## Option B : Int√©gration dans un cluster Spark existant

### √âtape 1 : R√©cup√©ration des JARs

```bash
# Cr√©e un dossier pour les JARs
mkdir -p /opt/spark-lineage/jars
cd /opt/spark-lineage/jars

# Download des JARs n√©cessaires
wget https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.33/mysql-connector-java-8.0.33.jar

# V√©rification
ls -la *.jar
```


### √âtape 2 : R√©cup√©ration du token OpenMetadata

```mermaid
sequenceDiagram
    participant Dev as üë®‚Äçüíª Toi
    participant OM as üìã OpenMetadata
    participant Bot as ü§ñ ingestion-bot
    
    Dev->>OM: Login sur :8585
    Dev->>OM: Settings ‚Üí Bots
    Dev->>Bot: Clique sur ingestion-bot
    Bot->>Dev: Page du bot
    Dev->>Bot: "Generate New Token"
    Bot->>Dev: üé´ JWT Token
    Dev->>Dev: Sauvegarde le token
```

1. Acc√©der √† OpenMetadata : `http://<openmetadata-host>:8585`
2. Aller dans Settings ‚Üí Bots
3. S√©lectionner ou cr√©er le bot d'ingestion
4. G√©n√©rer un nouveau token et le conserver dans un gestionnaire s√©curis√©


### √âtape 3 : Configuration Spark


#### M√©thode 1 : spark-submit (ex√©cution ponctuelle)

```bash
spark-submit \
  --master yarn \  # ou spark://master:7077, ou local[*]
  --deploy-mode cluster \
  --jars /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://ton-openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=TON_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=ton_service_pipeline" \
  --conf "spark.openmetadata.transport.pipelineName=ton_job_etl" \
  ton_job.py
```


#### M√©thode 2 : spark-defaults.conf (configuration globale)

```properties
# /opt/spark/conf/spark-defaults.conf
spark.jars                              /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar
spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener
spark.openmetadata.transport.type       openMetadata
spark.openmetadata.transport.hostPort   http://ton-openmetadata:8585/api
spark.openmetadata.transport.jwtToken   TON_JWT_TOKEN
spark.openmetadata.transport.pipelineServiceName ton_service_pipeline
```

#### M√©thode 3 : Configuration dans le code (Python/Scala)

**Python :**
```python
from pyspark.sql import SparkSession

# Configuration avec lineage
spark = SparkSession.builder \
    .appName("MonJobAvecLineage") \
    .config("spark.jars", "/opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar") \
    .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener") \
    .config("spark.openmetadata.transport.type", "openMetadata") \
    .config("spark.openmetadata.transport.hostPort", "http://ton-openmetadata:8585/api") \
    .config("spark.openmetadata.transport.jwtToken", "TON_JWT_TOKEN") \
    .config("spark.openmetadata.transport.pipelineServiceName", "mon_service") \
    .config("spark.openmetadata.transport.pipelineName", "mon_pipeline_etl") \
    .getOrCreate()

df_source = spark.read.jdbc("jdbc:mysql://source/db", "ma_table", properties=props)
df_clean = df_source.filter("status = 'active'").select("id", "name", "value")
df_clean.write.jdbc("jdbc:mysql://target/db", "ma_table_clean", mode="overwrite", properties=props)

spark.stop()

# Code ETL standard, le lineage est captur√© automatiquement
...existing code...
```

**Scala :**
```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("MonJobAvecLineage")
  .config("spark.jars", "/opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar")
  .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener")
  .config("spark.openmetadata.transport.type", "openMetadata")
  .config("spark.openmetadata.transport.hostPort", "http://ton-openmetadata:8585/api")
  .config("spark.openmetadata.transport.jwtToken", "TON_JWT_TOKEN")
  .config("spark.openmetadata.transport.pipelineServiceName", "mon_service")
  .getOrCreate()

// Ton ETL habituel
val dfSource = spark.read.jdbc("jdbc:mysql://source/db", "ma_table", props)
val dfClean = dfSource.filter($"status" === "active").select("id", "name", "value")
dfClean.write.jdbc("jdbc:mysql://target/db", "ma_table_clean", SaveMode.Overwrite, props)
```


#### M√©thode 4 : Variables d'environnement

```bash
# Variables pour simplifier
export SPARK_HOME=/opt/spark
export OPENMETADATA_HOST=http://ton-openmetadata:8585/api
export OPENMETADATA_JWT_TOKEN=ton_jwt_token
export PIPELINE_SERVICE_NAME=mon_service_pipeline

# Script de lancement simplifi√©
#!/bin/bash
$SPARK_HOME/bin/spark-submit \
  --jars /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=$OPENMETADATA_HOST" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=$PIPELINE_SERVICE_NAME" \
  $1  # Ton script Spark
```


### √âtape 4 : Configuration des services OpenMetadata

```mermaid
flowchart TD
    A[üìã OpenMetadata UI] --> B[Settings ‚Üí Services]
    B --> C[Databases ‚Üí Add Database Service]
    C --> D1[üóÑÔ∏è Service MySQL Source]
    C --> D2[üóÑÔ∏è Service MySQL Target] 
    C --> D3[üêò Service PostgreSQL]
    
    B --> E[Pipelines ‚Üí Add Pipeline Service]
    E --> F[‚ö° Service Spark Pipeline]
    
    D1 --> G[Test Connection]
    D2 --> G
    D3 --> G
    F --> H[Configure Service]
```

1. Cr√©er les services de bases de donn√©es dans OpenMetadata (Settings ‚Üí Services ‚Üí Databases)
2. Ajouter le service pipeline (Settings ‚Üí Services ‚Üí Pipelines), nom coh√©rent avec la configuration Spark

---


## Configuration avanc√©e

### üìä Options de Configuration Compl√®tes

```bash
# Configuration compl√®te pour spark-submit
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --jars /path/to/openmetadata-spark-agent.jar,/path/to/mysql-connector.jar \
  \
  # === OpenMetadata Core === 
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_JWT_TOKEN" \
  \
  # === Services & Pipeline ===
  --conf "spark.openmetadata.transport.pipelineServiceName=spark_pipeline_service" \
  --conf "spark.openmetadata.transport.pipelineName=mon_etl_pipeline" \
  --conf "spark.openmetadata.transport.pipelineDescription=Pipeline ETL automatique" \
  \
  # === Options Avanc√©es ===
  --conf "spark.openmetadata.transport.timeout=30" \
  --conf "spark.openmetadata.transport.includeInputs=true" \
  --conf "spark.openmetadata.transport.includeOutputs=true" \
  --conf "spark.openmetadata.transport.facetsDisabled=" \
  \
  # === Debug & Monitoring ===
  --conf "spark.openmetadata.transport.debugFacet=false" \
  --conf "spark.openmetadata.transport.metricsEnabled=true" \
  \
  ton_script.py
```

### üéØ Variables d'Environnement D√©taill√©es

```bash
# === Core OpenMetadata ===
export OPENMETADATA_HOST="http://ton-openmetadata:8585/api"
export OPENMETADATA_JWT_TOKEN="ton_super_token_jwt"

# === Services ===
export PIPELINE_SERVICE_NAME="spark_pipeline_service"
export DATABASE_SERVICE_SOURCE="mysql_source_service"  
export DATABASE_SERVICE_TARGET="mysql_target_service"

# === Pipeline Info ===
export PIPELINE_NAME="etl_${USER}_$(date +%Y%m%d)"
export PIPELINE_DESCRIPTION="Pipeline ETL automatique - $(date)"

# === Chemins ===
export SPARK_JARS_PATH="/opt/spark-lineage/jars"
export OPENMETADATA_JAR="$SPARK_JARS_PATH/openmetadata-spark-agent.jar"
export MYSQL_JAR="$SPARK_JARS_PATH/mysql-connector-j-8.0.33.jar"

# === Options ===
export LINEAGE_TIMEOUT=30
export LINEAGE_DEBUG=false
```


### Exemple de job Python complet

```python
#!/usr/bin/env python3
"""
Job Spark ETL avec Lineage OpenMetadata automatique
"""

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def create_spark_session():
    """Cr√©e une session Spark avec lineage OpenMetadata"""
    
    # Configuration depuis les variables d'environnement
    openmetadata_host = os.getenv('OPENMETADATA_HOST', 'http://localhost:8585/api')
    jwt_token = os.getenv('OPENMETADATA_JWT_TOKEN')
    pipeline_service = os.getenv('PIPELINE_SERVICE_NAME', 'spark_pipeline_service')
    pipeline_name = os.getenv('PIPELINE_NAME', f'etl_job_{os.getenv("USER", "spark")}')
    
    # V√©rification du token
    if not jwt_token:
        raise ValueError("OPENMETADATA_JWT_TOKEN non d√©fini !")
    
    # Chemins des JARs
    jars_path = os.getenv('SPARK_JARS_PATH', '/opt/spark-lineage/jars')
    openmetadata_jar = f"{jars_path}/openmetadata-spark-agent.jar"
    mysql_jar = f"{jars_path}/mysql-connector-j-8.0.33.jar"
    
    print(f"üöÄ Cr√©ation session Spark avec lineage vers {openmetadata_host}")
    print(f"üìä Pipeline Service: {pipeline_service}")
    print(f"‚ö° Pipeline Name: {pipeline_name}")
    
    return SparkSession.builder \
        .appName(f"ETL-{pipeline_name}") \
        .config("spark.jars", f"{openmetadata_jar},{mysql_jar}") \
        .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener") \
        .config("spark.openmetadata.transport.type", "openMetadata") \
        .config("spark.openmetadata.transport.hostPort", openmetadata_host) \
        .config("spark.openmetadata.transport.jwtToken", jwt_token) \
        .config("spark.openmetadata.transport.pipelineServiceName", pipeline_service) \
        .config("spark.openmetadata.transport.pipelineName", pipeline_name) \
        .config("spark.openmetadata.transport.timeout", "30") \
        .config("spark.openmetadata.transport.includeInputs", "true") \
        .config("spark.openmetadata.transport.includeOutputs", "true") \
        .getOrCreate()

def main():
    """Job ETL principal avec lineage automatique"""
    
    # Initialisation
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    print("üìä D√©but du job ETL avec lineage automatique...")
    
    try:
        # === Lecture des sources (lineage automatiquement captur√©) ===
        print("üìñ Lecture des donn√©es sources...")
        
        # Source MySQL
        df_customers = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://mysql-source:3306/ecommerce") \
            .option("dbtable", "customers") \
            .option("user", "root") \
            .option("password", "password") \
            .load()
        
        df_orders = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://mysql-source:3306/ecommerce") \
            .option("dbtable", "orders") \
            .option("user", "root") \
            .option("password", "password") \
            .load()
        
        # === Transformations (lineage des colonnes trac√©) ===
        print("üîÑ Transformations des donn√©es...")
        
        # Nettoyage et enrichissement
        df_customers_clean = df_customers \
            .filter(col("status") == "active") \
            .withColumn("full_name", concat(col("first_name"), lit(" "), col("last_name"))) \
            .select("customer_id", "full_name", "email", "country", "created_at")
        
        # Jointure et agr√©gation
        df_customer_stats = df_orders \
            .join(df_customers_clean, "customer_id") \
            .groupBy("customer_id", "full_name", "country") \
            .agg(
                count("order_id").alias("total_orders"),
                sum("amount").alias("total_spent"),
                max("order_date").alias("last_order_date")
            )
        
        # Calcul de segments client
        df_final = df_customer_stats \
            .withColumn("customer_segment", 
                when(col("total_spent") > 1000, "Premium")
                .when(col("total_spent") > 500, "Gold") 
                .otherwise("Standard")
            ) \
            .withColumn("processed_at", current_timestamp())
        
        # === √âcriture (lineage automatiquement captur√©) ===
        print("üíæ Sauvegarde des r√©sultats...")
        
        # Vers MySQL Target
        df_final.write \
            .format("jdbc") \
            .option("url", "jdbc:mysql://mysql-target:3306/analytics") \
            .option("dbtable", "customer_analytics") \
            .option("user", "root") \
            .option("password", "password") \
            .mode("overwrite") \
            .save()
        
        print(f"‚úÖ Job termin√© ! {df_final.count()} lignes trait√©es")
        print("üìã Lineage disponible dans OpenMetadata !")
        
    except Exception as e:
        print(f"‚ùå Erreur dans le job : {str(e)}")
        raise
    finally:
        spark.stop()
        print("üõë Session Spark ferm√©e")

if __name__ == "__main__":
    main()
```

### üöÄ Script de Lancement Automatis√©

```bash
#!/bin/bash
# run-spark-with-lineage.sh

set -e

# === Configuration ===
SCRIPT_NAME=$(basename "$1")
JOB_DATE=$(date +%Y%m%d_%H%M%S)

echo "üöÄ Lancement job Spark avec lineage : $SCRIPT_NAME"
echo "üìÖ Date/Heure : $JOB_DATE"

# V√©rifications pr√©alables
if [ -z "$OPENMETADATA_JWT_TOKEN" ]; then
    echo "‚ùå OPENMETADATA_JWT_TOKEN non d√©fini"
    exit 1
fi

if [ ! -f "$1" ]; then
    echo "‚ùå Script Spark non trouv√© : $1"
    exit 1
fi

# Export variables pour le job
export PIPELINE_NAME="etl_${USER}_${JOB_DATE}"
export PIPELINE_DESCRIPTION="Job ETL automatique - $SCRIPT_NAME - $JOB_DATE"

echo "üìä Pipeline : $PIPELINE_NAME"
echo "üîó OpenMetadata : $OPENMETADATA_HOST"

# Lancement avec lineage
$SPARK_HOME/bin/spark-submit \
  --master ${SPARK_MASTER:-yarn} \
  --deploy-mode ${SPARK_DEPLOY_MODE:-client} \
  --jars ${OPENMETADATA_JAR},${MYSQL_JAR} \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=$OPENMETADATA_HOST" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=$PIPELINE_SERVICE_NAME" \
  --conf "spark.openmetadata.transport.pipelineName=$PIPELINE_NAME" \
  --conf "spark.openmetadata.transport.pipelineDescription=$PIPELINE_DESCRIPTION" \
  "$@"

echo "‚úÖ Job termin√© ! Lineage disponible dans OpenMetadata"
```

**Utilisation :**
```bash
# Rendre le script ex√©cutable
chmod +x run-spark-with-lineage.sh

# Lancer ton job avec lineage
./run-spark-with-lineage.sh mon_job_etl.py

# Ou avec des options Spark suppl√©mentaires
./run-spark-with-lineage.sh mon_job_etl.py --conf "spark.executor.memory=4g"
```

---


## Exemples de configurations pr√™tes √† l'emploi

### üìã Pour YARN Cluster

```bash
# Production YARN avec lineage
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2g \
  --executor-memory 4g \
  --executor-cores 2 \
  --num-executors 10 \
  --jars hdfs://namenode:9000/spark-lineage/openmetadata-spark-agent.jar,hdfs://namenode:9000/spark-lineage/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata-prod:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=production_spark_service" \
  hdfs://namenode:9000/spark-jobs/production_etl.py
```

### üåä Pour Kubernetes

```yaml
# spark-lineage-k8s.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-lineage-config
data:
  spark-defaults.conf: |
    spark.jars                              /opt/spark/jars/openmetadata-spark-agent.jar,/opt/spark/jars/mysql-connector-j-8.0.33.jar
    spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener
    spark.openmetadata.transport.type       openMetadata
    spark.openmetadata.transport.hostPort   http://openmetadata-service:8585/api
    spark.openmetadata.transport.pipelineServiceName k8s_spark_service
---
apiVersion: batch/v1
kind: Job
metadata:
  name: spark-etl-with-lineage
spec:
  template:
    spec:
      containers:
      - name: spark-driver
        image: bitnami/spark:3.5.0
        env:
        - name: OPENMETADATA_JWT_TOKEN
          valueFrom:
            secretKeyRef:
              name: openmetadata-secret
              key: jwt-token
        volumeMounts:
        - name: spark-config
          mountPath: /opt/bitnami/spark/conf/spark-defaults.conf
          subPath: spark-defaults.conf
      volumes:
      - name: spark-config
        configMap:
          name: spark-lineage-config
      restartPolicy: Never
```

### üè† Pour Standalone Cluster

```bash
# Configuration Standalone
export SPARK_MASTER_URL="spark://master-node:7077"
export SPARK_HOME="/opt/spark"

# Lancement avec lineage
$SPARK_HOME/bin/spark-submit \
  --master $SPARK_MASTER_URL \
  --total-executor-cores 8 \
  --executor-memory 4g \
  --jars /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=standalone_spark_service" \
  ton_job.py
```

### üíª Pour D√©veloppement Local

```bash
# Mode local avec lineage pour debug
spark-submit \
  --master "local[*]" \
  --driver-memory 2g \
  --jars ./jars/openmetadata-spark-agent.jar,./jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://localhost:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=dev_spark_service" \
  --conf "spark.openmetadata.transport.debugFacet=true" \
  mon_job_dev.py
```

---


## D√©pannage et diagnostic


### Probl√®mes fr√©quents et solutions

```mermaid
flowchart TD
    Start[‚ùì Probl√®me Lineage] --> Check1{JARs pr√©sents ?}
    
    Check1 -->|‚ùå| Fix1[üì¶ T√©l√©charge les JARs]
    Check1 -->|‚úÖ| Check2{Token valide ?}
    
    Check2 -->|‚ùå| Fix2[üîë R√©g√©n√®re token OM]
    Check2 -->|‚úÖ| Check3{OM accessible ?}
    
    Check3 -->|‚ùå| Fix3[üåê Check connectivit√© r√©seau]
    Check3 -->|‚úÖ| Check4{Config Spark OK ?}
    
    Check4 -->|‚ùå| Fix4[‚öôÔ∏è V√©rifie spark.extraListeners]
    Check4 -->|‚úÖ| Check5{Logs d'erreur ?}
    
    Check5 -->|‚úÖ| Fix5[üêõ Mode debug activ√©]
    Check5 -->|‚ùå| Success[‚úÖ Myst√®re r√©solu !]
    
    Fix1 --> Test[üß™ Test √† nouveau]
    Fix2 --> Test
    Fix3 --> Test
    Fix4 --> Test
    Fix5 --> Test
```

#### üî• ClassNotFoundException : OpenLineageSparkListener

**Erreur :**
```
Exception in thread "main" java.lang.ClassNotFoundException: io.openlineage.spark.agent.OpenLineageSparkListener
```

**Solutions :**
```bash
# 1. V√©rifier que le JAR est pr√©sent
ls -la /path/to/openmetadata-spark-agent.jar

# 2. V√©rifier que le JAR est dans le classpath
spark-submit --jars /path/to/openmetadata-spark-agent.jar --conf "spark.driver.extraClassPath=/path/to/openmetadata-spark-agent.jar" ...

# 3. Re-t√©l√©charger le JAR si corrompu
wget https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar
```

#### üé´ Erreur Token JWT

**Erreur :**
```
HTTP 401: Unauthorized - Token verification failed
```

**Solutions :**
```bash
# 1. V√©rifier que le token est valide
curl -H "Authorization: Bearer $OPENMETADATA_JWT_TOKEN" http://openmetadata:8585/api/v1/system/version

# 2. R√©g√©n√©rer un nouveau token
# Via OpenMetadata UI : Settings ‚Üí Bots ‚Üí ingestion-bot ‚Üí Generate New Token

# 3. V√©rifier les variables d'environnement
echo "Token: ${OPENMETADATA_JWT_TOKEN:0:20}..."
```

#### üåê Probl√®mes de Connectivit√©

**Erreur :**
```
Connection refused to http://openmetadata:8585/api
```

**Solutions :**
```bash
# 1. Test de connectivit√©
telnet openmetadata-host 8585

# 2. V√©rifier l'URL dans la config
# Attention : /api √† la fin est obligatoire !
spark.openmetadata.transport.hostPort=http://openmetadata:8585/api

# 3. R√©solution DNS (si en Docker)
nslookup openmetadata
```

#### ‚öôÔ∏è Config Spark Non Prise en Compte

**Si le lineage ne marche pas :**

```bash
# 1. V√©rifier que le listener est bien configur√©
spark-submit --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" ...

# 2. V√©rifier les logs Spark
tail -f $SPARK_HOME/logs/spark-*.log | grep -i openlineage

# 3. Activer le mode debug
--conf "spark.openmetadata.transport.debugFacet=true"
```

### üêõ Mode Debug Complet

```bash
# Lancement avec debug maximal
spark-submit \
  --master local[*] \
  --jars ./jars/openmetadata-spark-agent.jar,./jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://localhost:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=debug_service" \
  --conf "spark.openmetadata.transport.debugFacet=true" \
  --conf "spark.sql.execution.arrow.pyspark.enabled=false" \
  --driver-java-options "-Dlog4j.configuration=file:log4j.properties" \
  ton_job.py 2>&1 | tee debug_lineage.log

# Analyser les logs
grep -i "openlineage\|openmetadata" debug_lineage.log
```


### Checklist de diagnostic

1. **‚úÖ JARs** : `ls -la *.jar` 
2. **‚úÖ Token** : `curl -H "Authorization: Bearer $TOKEN" http://om:8585/api/v1/system/version`
3. **‚úÖ R√©seau** : `telnet openmetadata-host 8585`
4. **‚úÖ Config** : V√©rifier `spark.extraListeners`
5. **‚úÖ Services OM** : Pipeline service cr√©√© dans OpenMetadata
6. **‚úÖ Logs** : Chercher "openlineage" dans les logs Spark

---


## R√©sultats attendus dans OpenMetadata

### üéØ Ce que tu vas voir

```mermaid
graph TB
    subgraph OM[üìã OpenMetadata Interface]
        
        subgraph Services[üè¢ Services]
            PS[‚ö° Pipeline Service: Spark]
            DBS[üóÑÔ∏è Database Service: MySQL Source]
            DBT[üóÑÔ∏è Database Service: MySQL Target]
        end
        
        subgraph Pipelines[üîÑ Pipelines]
            P1[üìù mon_etl_pipeline]
            P2[üìä customer_analytics_pipeline] 
            P3[üîÑ daily_processing_pipeline]
        end
        
        subgraph Lineage[üîó Data Lineage]
            L1[üìä Table customers] --> L2[üîÑ Spark Transform]
            L3[üìä Table orders] --> L2
            L2 --> L4[üìà Table customer_analytics]
        end
        
        subgraph Metadata[üìã M√©tadonn√©es]
            M1[üìä Sch√©mas]
            M2[üè∑Ô∏è Colonnes] 
            M3[üîç Types de donn√©es]
            M4[‚è±Ô∏è Historique ex√©cutions]
        end
    end
    
    Services --> Pipelines
    Pipelines --> Lineage
    Lineage --> Metadata
```


### Lineage Graph interactif

Dans OpenMetadata, vous visualiserez :
- Un graphe de lineage entre tables sources, transformations Spark et tables cibles
- La tra√ßabilit√© colonne √† colonne
- L'historique d'ex√©cution des pipelines
- Les m√©triques d'ex√©cution (lignes trait√©es, dur√©e, statut)
- Les tags automatiques (Spark, ETL, nom du pipeline)

---

## üá¨üáß English

### üìã Quick Start Guide

This project provides automatic data lineage tracking for Apache Spark jobs with OpenMetadata integration. Perfect for production environments where you need to track ETL pipelines automatically.

### ‚ö° Key Features
- ‚úÖ **Zero-code lineage** - Just add JARs and configuration
- ‚úÖ **Real-time tracking** - Captures lineage as jobs execute
- ‚úÖ **Multi-platform** - Works on YARN, Kubernetes, Standalone
- ‚úÖ **Column-level lineage** - Tracks transformations at column level
- ‚úÖ **Production ready** - Used in enterprise environments

### ÔøΩ Quick Integration

**For existing Spark environments:**

```bash
# 1. Download required JARs
wget https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar

# 2. Add to your spark-submit
spark-submit \
  --jars openmetadata-spark-agent.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://your-openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_JWT_TOKEN" \
  your_existing_job.py
```

**For Docker testing:**

```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
docker-compose up -d
./run-example.sh
```

### üìä Access Points
- **OpenMetadata UI**: http://localhost:8585 (admin/admin)
- **Spark Master UI**: http://localhost:8080
- **Example Databases**: MySQL on ports 3307/3308

---

## üá™üá∏ Espa√±ol

### üìã Descripci√≥n
Este proyecto implementa un sistema de linaje autom√°tico para Apache Spark con OpenMetadata. Rastrea autom√°ticamente los flujos de datos entre tablas MySQL a trav√©s de transformaciones ETL de Spark.

### ‚ö° Caracter√≠sticas
- ‚úÖ **Linaje autom√°tico** - Captura autom√°tica de relaciones entre tablas
- ‚úÖ **Multi-tabla** - Soporte para transformaciones complejas (7 tablas: 4 fuentes + 3 objetivos)
- ‚úÖ **OpenMetadata** - Integraci√≥n nativa con OpenMetadata 1.9.7
- ‚úÖ **Docker** - Entorno completo con Docker Compose
- ‚úÖ **Spark 3.5.0** - √öltima versi√≥n estable de Apache Spark

### üõ†Ô∏è Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
```

2. **Iniciar entorno Docker**
```bash
docker-compose up -d
```

3. **Ejecutar trabajo Spark**
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf "spark.driver.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  --conf "spark.executor.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### üìä Servicios
- **OpenMetadata UI**: http://localhost:8585
- **Spark Master UI**: http://localhost:8080
- **MySQL Fuente**: localhost:3308 (root/password)
- **MySQL Objetivo**: localhost:3307 (root/password)

---

## üáµüáπ Portugu√™s

### üìã Descri√ß√£o
Este projeto implementa um sistema de linhagem autom√°tica para Apache Spark com OpenMetadata. Rastreia automaticamente os fluxos de dados entre tabelas MySQL atrav√©s de transforma√ß√µes ETL do Spark.

### ‚ö° Funcionalidades
- ‚úÖ **Linhagem autom√°tica** - Captura autom√°tica de relacionamentos entre tabelas
- ‚úÖ **Multi-tabela** - Suporte para transforma√ß√µes complexas (7 tabelas: 4 fontes + 3 destinos)
- ‚úÖ **OpenMetadata** - Integra√ß√£o nativa com OpenMetadata 1.9.7
- ‚úÖ **Docker** - Ambiente completo com Docker Compose
- ‚úÖ **Spark 3.5.0** - √öltima vers√£o est√°vel do Apache Spark

### üõ†Ô∏è Instala√ß√£o

1. **Clonar o reposit√≥rio**
```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
```

2. **Iniciar ambiente Docker**
```bash
docker-compose up -d
```

3. **Executar job Spark**
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf "spark.driver.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  --conf "spark.executor.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### üìä Servi√ßos
- **OpenMetadata UI**: http://localhost:8585
- **Spark Master UI**: http://localhost:8080
- **MySQL Fonte**: localhost:3308 (root/password)
- **MySQL Destino**: localhost:3307 (root/password)

---

## üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©

### üìã ÿßŸÑŸàÿµŸÅ
Ÿäÿ∑ÿ®ŸÇ Ÿáÿ∞ÿß ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ ŸÜÿ∏ÿßŸÖ ŸÜÿ≥ÿ® ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ™ŸÑŸÇÿßÿ¶Ÿä ŸÑŸÄ Apache Spark ŸÖÿπ OpenMetadata. Ÿäÿ™ÿ™ÿ®ÿπ ÿ™ŸÑŸÇÿßÿ¶ŸäÿßŸã ÿ™ÿØŸÅŸÇÿßÿ™ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ®ŸäŸÜ ÿ¨ÿØÿßŸàŸÑ MySQL ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿ™ÿ≠ŸàŸäŸÑÿßÿ™ Spark ETL.

### ‚ö° ÿßŸÑŸÖŸäÿ≤ÿßÿ™
- ‚úÖ **ŸÜÿ≥ÿ® ÿ™ŸÑŸÇÿßÿ¶Ÿä** - ÿßŸÑÿ™ŸÇÿßÿ∑ ÿ™ŸÑŸÇÿßÿ¶Ÿä ŸÑÿπŸÑÿßŸÇÿßÿ™ ÿßŸÑÿ¨ÿØÿßŸàŸÑ
- ‚úÖ **ŸÖÿ™ÿπÿØÿØ ÿßŸÑÿ¨ÿØÿßŸàŸÑ** - ÿØÿπŸÖ ŸÑŸÑÿ™ÿ≠ŸàŸäŸÑÿßÿ™ ÿßŸÑŸÖÿπŸÇÿØÿ© (7 ÿ¨ÿØÿßŸàŸÑ: 4 ŸÖÿµÿßÿØÿ± + 3 ÿ£ŸáÿØÿßŸÅ)
- ‚úÖ **OpenMetadata** - ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ÿßŸÑÿ£ÿµŸÑŸä ŸÖÿπ OpenMetadata 1.9.7
- ‚úÖ **Docker** - ÿ®Ÿäÿ¶ÿ© ŸÉÿßŸÖŸÑÿ© ŸÖÿπ Docker Compose
- ‚úÖ **Spark 3.5.0** - ÿ£ÿ≠ÿØÿ´ ÿ•ÿµÿØÿßÿ± ŸÖÿ≥ÿ™ŸÇÿ± ŸÖŸÜ Apache Spark

### üõ†Ô∏è ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™

**1. ÿßÿ≥ÿ™ŸÜÿ≥ÿßÿÆ ÿßŸÑŸÖÿ≥ÿ™ŸàÿØÿπ**
```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
```

**2. ÿ®ÿØÿ° ÿ®Ÿäÿ¶ÿ© Docker**
```bash
docker-compose up -d
```

**3. ÿ™ÿ¥ÿ∫ŸäŸÑ ŸÖŸáŸÖÿ© Spark**
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf "spark.driver.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  --conf "spark.executor.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### üìä ÿßŸÑÿÆÿØŸÖÿßÿ™
- **OpenMetadata UI**: http://localhost:8585
- **Spark Master UI**: http://localhost:8080
- **MySQL ÿßŸÑŸÖÿµÿØÿ±**: localhost:3308 (root/password)
- **MySQL ÿßŸÑŸáÿØŸÅ**: localhost:3307 (root/password)

---

## üèóÔ∏è Architecture | Arquitectura | Arquitetura | ÿßŸÑŸáŸÜÿØÿ≥ÿ© ÿßŸÑŸÖÿπŸÖÿßÿ±Ÿäÿ©

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MySQL Source  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Apache Spark  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  MySQL Target   ‚îÇ
‚îÇ   (Port 3308)   ‚îÇ    ‚îÇ   ETL Process   ‚îÇ    ‚îÇ   (Port 3307)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  OpenMetadata   ‚îÇ
                    ‚îÇ   (Port 8585)   ‚îÇ
                    ‚îÇ Lineage Tracking‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure | Estructura | Estrutura | ŸáŸäŸÉŸÑ ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ

```
sparklineage/
‚îú‚îÄ‚îÄ complex_spark_lineage_job.py    # Main Spark ETL job
‚îú‚îÄ‚îÄ docker-compose.yml              # Docker services configuration
‚îú‚îÄ‚îÄ samples/                        # SQL initialization examples
‚îÇ   ‚îú‚îÄ‚îÄ init-source.sql
‚îÇ   ‚îú‚îÄ‚îÄ init-target.sql
‚îÇ   ‚îî‚îÄ‚îÄ create_*.sql
‚îú‚îÄ‚îÄ jars/                           # Required JAR files
‚îÇ   ‚îú‚îÄ‚îÄ openmetadata-spark-agent.jar
‚îÇ   ‚îî‚îÄ‚îÄ mysql-connector-j-8.0.33.jar
‚îî‚îÄ‚îÄ README.md                       # This file
```

## ü§ù Contributing | Contribuir | Contribuindo | ÿßŸÑŸÖÿ≥ÿßŸáŸÖÿ©

1. Fork the project | Haz fork del proyecto | Fa√ßa fork do projeto | ŸÇŸÖ ÿ®ÿπŸÖŸÑ fork ŸÑŸÑŸÖÿ¥ÿ±Ÿàÿπ
2. Create your feature branch | Crea tu rama de caracter√≠sticas | Crie sua branch de feature | ÿ£ŸÜÿ¥ÿ¶ ŸÅÿ±ÿπ ÿßŸÑŸÖŸäÿ≤ÿ© ÿßŸÑÿÆÿßÿµ ÿ®ŸÉ
3. Commit your changes | Haz commit de tus cambios | Fa√ßa commit das suas mudan√ßas | ŸÇŸÖ ÿ®ÿπŸÖŸÑ commit ŸÑÿ™ÿ∫ŸäŸäÿ±ÿßÿ™ŸÉ
4. Push to the branch | Haz push a la rama | Fa√ßa push para a branch | ÿßÿØŸÅÿπ ÿ•ŸÑŸâ ÿßŸÑŸÅÿ±ÿπ
5. Open a Pull Request | Abre un Pull Request | Abra um Pull Request | ÿßŸÅÿ™ÿ≠ Pull Request

## üìù License | Licencia | Licen√ßa | ÿßŸÑÿ™ÿ±ÿÆŸäÿµ

This project is licensed under the MIT License - see the LICENSE file for details.

---

