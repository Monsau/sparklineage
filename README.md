# ---
# English Translation

## Spark + OpenMetadata | Automatic Data Lineage

This project provides an integration solution for automatic data lineage tracking in Apache Spark, with direct exposure in OpenMetadata. It is intended for technical teams wishing to industrialize the traceability of their Spark pipelines without modifying application code.

...existing code...

# ---
# TraducciÃ³n al EspaÃ±ol

## Spark + OpenMetadata | Linaje de Datos AutomÃ¡tico

Este proyecto proporciona una soluciÃ³n de integraciÃ³n para el seguimiento automÃ¡tico del linaje de datos en Apache Spark, con exposiciÃ³n directa en OpenMetadata. EstÃ¡ destinado a equipos tÃ©cnicos que deseen industrializar la trazabilidad de sus pipelines de Spark sin modificar el cÃ³digo de la aplicaciÃ³n.

...existing code...

# ---
# Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

## Ø³Ø¨Ø§Ø±Ùƒ + Ø£ÙˆØ¨Ù† Ù…ÙŠØªØ§Ø¯Ø§ØªØ§ | ØªØªØ¨Ø¹ Ù†Ø³Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ

ÙŠÙˆÙØ± Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø­Ù„Ø§Ù‹ Ù„Ù„ØªÙƒØ§Ù…Ù„ Ù„ØªØªØ¨Ø¹ Ù†Ø³Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙÙŠ Apache SparkØŒ Ù…Ø¹ Ø¹Ø±Ø¶ Ù…Ø¨Ø§Ø´Ø± ÙÙŠ OpenMetadata. ÙˆÙ‡Ùˆ Ù…Ø®ØµØµ Ù„Ù„ÙØ±Ù‚ Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø£ØªÙ…ØªØ© ØªØªØ¨Ø¹ Ø®Ø·ÙˆØ· Ø£Ù†Ø§Ø¨ÙŠØ¨ Spark Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù‡Ù… Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„ ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.

...existing code...

# Spark + OpenMetadata | Lineage Automatique de DonnÃ©es

Ce projet fournit une solution d'intÃ©gration pour le suivi automatique du lineage des donnÃ©es dans Apache Spark, avec exposition directe dans OpenMetadata. Il s'adresse aux Ã©quipes techniques souhaitant industrialiser la traÃ§abilitÃ© de leurs pipelines Spark, sans modification du code applicatif.


## FonctionnalitÃ©s principales

```mermaid
flowchart LR
    subgraph Sources[ğŸ“Š Sources de DonnÃ©es]
        S1[ğŸ—„ï¸ MySQL Source]
        S2[ğŸ“ Fichiers CSV] 
        S3[ğŸª£ S3 Bucket]
        S4[ğŸ˜ PostgreSQL]
    end
    
    subgraph Spark[âš¡ Apache Spark]
        J1[ğŸ“ Job ETL]
        J2[ğŸ”„ Transformations]
        J3[ğŸ“Š AgrÃ©gations]
    end
    
    subgraph Targets[ğŸ¯ Destinations]
        T1[ğŸ—„ï¸ MySQL Target]
        T2[ğŸ“Š Data Warehouse]
        T3[ğŸ“ˆ Analytics DB]
    end
    
    subgraph OM[ğŸ“‹ OpenMetadata]
        L1[ğŸ”— Lineage Graph]
        L2[ï¿½ Data Quality]
        L3[â±ï¸ Pipeline History]
    end
    
    Sources --> Spark
    Spark --> Targets
    Spark -.->|Auto-track| OM
    
    style Spark fill:#ff6b35
    style OM fill:#4ecdc4
```

- DÃ©couverte automatique des sources et destinations
- Suivi du lineage en temps rÃ©el pour chaque transformation Spark
- Visualisation graphique dans OpenMetadata
- IntÃ©gration sans modification du code applicatif (configuration uniquement)
- MÃ©tadonnÃ©es enrichies (schÃ©mas, colonnes, transformations)


## Modes d'intÃ©gration


### Option A : Environnement complet (Docker)

```mermaid
graph TB
    subgraph Docker[ğŸ³ Docker Compose]
        D1[ğŸ“Š OpenMetadata]
        D2[âš¡ Spark Master]
        D3[ğŸ”§ Spark Worker] 
        D4[ğŸ—„ï¸ MySQL Source]
        D5[ğŸ—„ï¸ MySQL Target]
    end
    
    D2 --> D3
    D2 -.->|Lineage| D1
    D4 --> D2
    D2 --> D5
```


### Option B : IntÃ©gration dans un cluster Spark existant

```mermaid
graph LR
    subgraph Existing[ğŸ¢ Ton Infrastructure]
        E1[âš¡ Spark Cluster]
        E2[ğŸ“Š Tes DBs]
        E3[ğŸ”„ Tes Jobs ETL]
    end
    
    subgraph Add[â• Ã€ Ajouter]
        A1[ğŸ“‹ OpenMetadata]
        A2[ğŸ”§ Agent JAR]
        A3[âš™ï¸ Config Spark]
    end
    
    Existing --> Add
    Add -.->|Lineage Auto| A1
```

---


## Option A : DÃ©ploiement Docker

### Installation rapide

```bash
# 1. Clone le projet
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage

# 2. Lance tout l'environnement
docker-compose up -d

# 3. Attends que tout dÃ©marre (30 sec environ)
docker-compose ps

# 4. Lance le job d'exemple
./run-example.sh
```

#### Lancement manuel
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/bitnami/spark/jars/openmetadata-spark-agent.jar,/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_TOKEN" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### ğŸ“Š Services Disponibles

| Service | URL | Credentials |
|---------|-----|-------------|
| ğŸ“‹ **OpenMetadata UI** | http://localhost:8585 | admin/admin |
| âš¡ **Spark Master** | http://localhost:8080 | - |
| ğŸ—„ï¸ **MySQL Source** | localhost:3308 | root/password |
| ğŸ—„ï¸ **MySQL Target** | localhost:3307 | root/password |

---


## Option B : IntÃ©gration dans un cluster Spark existant

### Ã‰tape 1 : RÃ©cupÃ©ration des JARs

```bash
# CrÃ©e un dossier pour les JARs
mkdir -p /opt/spark-lineage/jars
cd /opt/spark-lineage/jars

# Download des JARs nÃ©cessaires
wget https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.33/mysql-connector-java-8.0.33.jar

# VÃ©rification
ls -la *.jar
```


### Ã‰tape 2 : RÃ©cupÃ©ration du token OpenMetadata

```mermaid
sequenceDiagram
    participant Dev as ğŸ‘¨â€ğŸ’» Toi
    participant OM as ğŸ“‹ OpenMetadata
    participant Bot as ğŸ¤– ingestion-bot
    
    Dev->>OM: Login sur :8585
    Dev->>OM: Settings â†’ Bots
    Dev->>Bot: Clique sur ingestion-bot
    Bot->>Dev: Page du bot
    Dev->>Bot: "Generate New Token"
    Bot->>Dev: ğŸ« JWT Token
    Dev->>Dev: Sauvegarde le token
```

1. AccÃ©der Ã  OpenMetadata : `http://<openmetadata-host>:8585`
2. Aller dans Settings â†’ Bots
3. SÃ©lectionner ou crÃ©er le bot d'ingestion
4. GÃ©nÃ©rer un nouveau token et le conserver dans un gestionnaire sÃ©curisÃ©


### Ã‰tape 3 : Configuration Spark


#### MÃ©thode 1 : spark-submit (exÃ©cution ponctuelle)

```bash
spark-submit \
  --master yarn \  # ou spark://master:7077, ou local[*]
  --deploy-mode cluster \
  --jars /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://ton-openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=TON_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=ton_service_pipeline" \
  --conf "spark.openmetadata.transport.pipelineName=ton_job_etl" \
  ton_job.py
```


#### MÃ©thode 2 : spark-defaults.conf (configuration globale)

```properties
# /opt/spark/conf/spark-defaults.conf
spark.jars                              /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar
spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener
spark.openmetadata.transport.type       openMetadata
spark.openmetadata.transport.hostPort   http://ton-openmetadata:8585/api
spark.openmetadata.transport.jwtToken   TON_JWT_TOKEN
spark.openmetadata.transport.pipelineServiceName ton_service_pipeline
```

#### MÃ©thode 3 : Configuration dans le code (Python/Scala)

**Python :**
```python
from pyspark.sql import SparkSession

# Configuration avec lineage
spark = SparkSession.builder \
    .appName("MonJobAvecLineage") \
    .config("spark.jars", "/opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar") \
    .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener") \
    .config("spark.openmetadata.transport.type", "openMetadata") \
    .config("spark.openmetadata.transport.hostPort", "http://ton-openmetadata:8585/api") \
    .config("spark.openmetadata.transport.jwtToken", "TON_JWT_TOKEN") \
    .config("spark.openmetadata.transport.pipelineServiceName", "mon_service") \
    .config("spark.openmetadata.transport.pipelineName", "mon_pipeline_etl") \
    .getOrCreate()

df_source = spark.read.jdbc("jdbc:mysql://source/db", "ma_table", properties=props)
df_clean = df_source.filter("status = 'active'").select("id", "name", "value")
df_clean.write.jdbc("jdbc:mysql://target/db", "ma_table_clean", mode="overwrite", properties=props)

spark.stop()

# Code ETL standard, le lineage est capturÃ© automatiquement
...existing code...
```

**Scala :**
```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("MonJobAvecLineage")
  .config("spark.jars", "/opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar")
  .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener")
  .config("spark.openmetadata.transport.type", "openMetadata")
  .config("spark.openmetadata.transport.hostPort", "http://ton-openmetadata:8585/api")
  .config("spark.openmetadata.transport.jwtToken", "TON_JWT_TOKEN")
  .config("spark.openmetadata.transport.pipelineServiceName", "mon_service")
  .getOrCreate()

// Ton ETL habituel
val dfSource = spark.read.jdbc("jdbc:mysql://source/db", "ma_table", props)
val dfClean = dfSource.filter($"status" === "active").select("id", "name", "value")
dfClean.write.jdbc("jdbc:mysql://target/db", "ma_table_clean", SaveMode.Overwrite, props)
```


#### MÃ©thode 4 : Variables d'environnement

```bash
# Variables pour simplifier
export SPARK_HOME=/opt/spark
export OPENMETADATA_HOST=http://ton-openmetadata:8585/api
export OPENMETADATA_JWT_TOKEN=ton_jwt_token
export PIPELINE_SERVICE_NAME=mon_service_pipeline

# Script de lancement simplifiÃ©
#!/bin/bash
$SPARK_HOME/bin/spark-submit \
  --jars /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=$OPENMETADATA_HOST" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=$PIPELINE_SERVICE_NAME" \
  $1  # Ton script Spark
```


### Ã‰tape 4 : Configuration des services OpenMetadata

```mermaid
flowchart TD
    A[ğŸ“‹ OpenMetadata UI] --> B[Settings â†’ Services]
    B --> C[Databases â†’ Add Database Service]
    C --> D1[ğŸ—„ï¸ Service MySQL Source]
    C --> D2[ğŸ—„ï¸ Service MySQL Target] 
    C --> D3[ğŸ˜ Service PostgreSQL]
    
    B --> E[Pipelines â†’ Add Pipeline Service]
    E --> F[âš¡ Service Spark Pipeline]
    
    D1 --> G[Test Connection]
    D2 --> G
    D3 --> G
    F --> H[Configure Service]
```

1. CrÃ©er les services de bases de donnÃ©es dans OpenMetadata (Settings â†’ Services â†’ Databases)
2. Ajouter le service pipeline (Settings â†’ Services â†’ Pipelines), nom cohÃ©rent avec la configuration Spark

---


## Configuration avancÃ©e

### ğŸ“Š Options de Configuration ComplÃ¨tes

```bash
# Configuration complÃ¨te pour spark-submit
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --jars /path/to/openmetadata-spark-agent.jar,/path/to/mysql-connector.jar \
  \
  # === OpenMetadata Core === 
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_JWT_TOKEN" \
  \
  # === Services & Pipeline ===
  --conf "spark.openmetadata.transport.pipelineServiceName=spark_pipeline_service" \
  --conf "spark.openmetadata.transport.pipelineName=mon_etl_pipeline" \
  --conf "spark.openmetadata.transport.pipelineDescription=Pipeline ETL automatique" \
  \
  # === Options AvancÃ©es ===
  --conf "spark.openmetadata.transport.timeout=30" \
  --conf "spark.openmetadata.transport.includeInputs=true" \
  --conf "spark.openmetadata.transport.includeOutputs=true" \
  --conf "spark.openmetadata.transport.facetsDisabled=" \
  \
  # === Debug & Monitoring ===
  --conf "spark.openmetadata.transport.debugFacet=false" \
  --conf "spark.openmetadata.transport.metricsEnabled=true" \
  \
  ton_script.py
```

### ğŸ¯ Variables d'Environnement DÃ©taillÃ©es

```bash
# === Core OpenMetadata ===
export OPENMETADATA_HOST="http://ton-openmetadata:8585/api"
export OPENMETADATA_JWT_TOKEN="ton_super_token_jwt"

# === Services ===
export PIPELINE_SERVICE_NAME="spark_pipeline_service"
export DATABASE_SERVICE_SOURCE="mysql_source_service"  
export DATABASE_SERVICE_TARGET="mysql_target_service"

# === Pipeline Info ===
export PIPELINE_NAME="etl_${USER}_$(date +%Y%m%d)"
export PIPELINE_DESCRIPTION="Pipeline ETL automatique - $(date)"

# === Chemins ===
export SPARK_JARS_PATH="/opt/spark-lineage/jars"
export OPENMETADATA_JAR="$SPARK_JARS_PATH/openmetadata-spark-agent.jar"
export MYSQL_JAR="$SPARK_JARS_PATH/mysql-connector-j-8.0.33.jar"

# === Options ===
export LINEAGE_TIMEOUT=30
export LINEAGE_DEBUG=false
```


### Exemple de job Python complet

```python
#!/usr/bin/env python3
"""
Job Spark ETL avec Lineage OpenMetadata automatique
"""

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def create_spark_session():
    """CrÃ©e une session Spark avec lineage OpenMetadata"""
    
    # Configuration depuis les variables d'environnement
    openmetadata_host = os.getenv('OPENMETADATA_HOST', 'http://localhost:8585/api')
    jwt_token = os.getenv('OPENMETADATA_JWT_TOKEN')
    pipeline_service = os.getenv('PIPELINE_SERVICE_NAME', 'spark_pipeline_service')
    pipeline_name = os.getenv('PIPELINE_NAME', f'etl_job_{os.getenv("USER", "spark")}')
    
    # VÃ©rification du token
    if not jwt_token:
        raise ValueError("OPENMETADATA_JWT_TOKEN non dÃ©fini !")
    
    # Chemins des JARs
    jars_path = os.getenv('SPARK_JARS_PATH', '/opt/spark-lineage/jars')
    openmetadata_jar = f"{jars_path}/openmetadata-spark-agent.jar"
    mysql_jar = f"{jars_path}/mysql-connector-j-8.0.33.jar"
    
    print(f"ğŸš€ CrÃ©ation session Spark avec lineage vers {openmetadata_host}")
    print(f"ğŸ“Š Pipeline Service: {pipeline_service}")
    print(f"âš¡ Pipeline Name: {pipeline_name}")
    
    return SparkSession.builder \
        .appName(f"ETL-{pipeline_name}") \
        .config("spark.jars", f"{openmetadata_jar},{mysql_jar}") \
        .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener") \
        .config("spark.openmetadata.transport.type", "openMetadata") \
        .config("spark.openmetadata.transport.hostPort", openmetadata_host) \
        .config("spark.openmetadata.transport.jwtToken", jwt_token) \
        .config("spark.openmetadata.transport.pipelineServiceName", pipeline_service) \
        .config("spark.openmetadata.transport.pipelineName", pipeline_name) \
        .config("spark.openmetadata.transport.timeout", "30") \
        .config("spark.openmetadata.transport.includeInputs", "true") \
        .config("spark.openmetadata.transport.includeOutputs", "true") \
        .getOrCreate()

def main():
    """Job ETL principal avec lineage automatique"""
    
    # Initialisation
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    print("ğŸ“Š DÃ©but du job ETL avec lineage automatique...")
    
    try:
        # === Lecture des sources (lineage automatiquement capturÃ©) ===
        print("ğŸ“– Lecture des donnÃ©es sources...")
        
        # Source MySQL
        df_customers = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://mysql-source:3306/ecommerce") \
            .option("dbtable", "customers") \
            .option("user", "root") \
            .option("password", "password") \
            .load()
        
        df_orders = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://mysql-source:3306/ecommerce") \
            .option("dbtable", "orders") \
            .option("user", "root") \
            .option("password", "password") \
            .load()
        
        # === Transformations (lineage des colonnes tracÃ©) ===
        print("ğŸ”„ Transformations des donnÃ©es...")
        
        # Nettoyage et enrichissement
        df_customers_clean = df_customers \
            .filter(col("status") == "active") \
            .withColumn("full_name", concat(col("first_name"), lit(" "), col("last_name"))) \
            .select("customer_id", "full_name", "email", "country", "created_at")
        
        # Jointure et agrÃ©gation
        df_customer_stats = df_orders \
            .join(df_customers_clean, "customer_id") \
            .groupBy("customer_id", "full_name", "country") \
            .agg(
                count("order_id").alias("total_orders"),
                sum("amount").alias("total_spent"),
                max("order_date").alias("last_order_date")
            )
        
        # Calcul de segments client
        df_final = df_customer_stats \
            .withColumn("customer_segment", 
                when(col("total_spent") > 1000, "Premium")
                .when(col("total_spent") > 500, "Gold") 
                .otherwise("Standard")
            ) \
            .withColumn("processed_at", current_timestamp())
        
        # === Ã‰criture (lineage automatiquement capturÃ©) ===
        print("ğŸ’¾ Sauvegarde des rÃ©sultats...")
        
        # Vers MySQL Target
        df_final.write \
            .format("jdbc") \
            .option("url", "jdbc:mysql://mysql-target:3306/analytics") \
            .option("dbtable", "customer_analytics") \
            .option("user", "root") \
            .option("password", "password") \
            .mode("overwrite") \
            .save()
        
        print(f"âœ… Job terminÃ© ! {df_final.count()} lignes traitÃ©es")
        print("ğŸ“‹ Lineage disponible dans OpenMetadata !")
        
    except Exception as e:
        print(f"âŒ Erreur dans le job : {str(e)}")
        raise
    finally:
        spark.stop()
        print("ğŸ›‘ Session Spark fermÃ©e")

if __name__ == "__main__":
    main()
```

### ğŸš€ Script de Lancement AutomatisÃ©

```bash
#!/bin/bash
# run-spark-with-lineage.sh

set -e

# === Configuration ===
SCRIPT_NAME=$(basename "$1")
JOB_DATE=$(date +%Y%m%d_%H%M%S)

echo "ğŸš€ Lancement job Spark avec lineage : $SCRIPT_NAME"
echo "ğŸ“… Date/Heure : $JOB_DATE"

# VÃ©rifications prÃ©alables
if [ -z "$OPENMETADATA_JWT_TOKEN" ]; then
    echo "âŒ OPENMETADATA_JWT_TOKEN non dÃ©fini"
    exit 1
fi

if [ ! -f "$1" ]; then
    echo "âŒ Script Spark non trouvÃ© : $1"
    exit 1
fi

# Export variables pour le job
export PIPELINE_NAME="etl_${USER}_${JOB_DATE}"
export PIPELINE_DESCRIPTION="Job ETL automatique - $SCRIPT_NAME - $JOB_DATE"

echo "ğŸ“Š Pipeline : $PIPELINE_NAME"
echo "ğŸ”— OpenMetadata : $OPENMETADATA_HOST"

# Lancement avec lineage
$SPARK_HOME/bin/spark-submit \
  --master ${SPARK_MASTER:-yarn} \
  --deploy-mode ${SPARK_DEPLOY_MODE:-client} \
  --jars ${OPENMETADATA_JAR},${MYSQL_JAR} \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=$OPENMETADATA_HOST" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=$PIPELINE_SERVICE_NAME" \
  --conf "spark.openmetadata.transport.pipelineName=$PIPELINE_NAME" \
  --conf "spark.openmetadata.transport.pipelineDescription=$PIPELINE_DESCRIPTION" \
  "$@"

echo "âœ… Job terminÃ© ! Lineage disponible dans OpenMetadata"
```

**Utilisation :**
```bash
# Rendre le script exÃ©cutable
chmod +x run-spark-with-lineage.sh

# Lancer ton job avec lineage
./run-spark-with-lineage.sh mon_job_etl.py

# Ou avec des options Spark supplÃ©mentaires
./run-spark-with-lineage.sh mon_job_etl.py --conf "spark.executor.memory=4g"
```

---


## Exemples de configurations prÃªtes Ã  l'emploi

### ğŸ“‹ Pour YARN Cluster

```bash
# Production YARN avec lineage
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2g \
  --executor-memory 4g \
  --executor-cores 2 \
  --num-executors 10 \
  --jars hdfs://namenode:9000/spark-lineage/openmetadata-spark-agent.jar,hdfs://namenode:9000/spark-lineage/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata-prod:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=production_spark_service" \
  hdfs://namenode:9000/spark-jobs/production_etl.py
```

### ğŸŒŠ Pour Kubernetes

```yaml
# spark-lineage-k8s.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-lineage-config
data:
  spark-defaults.conf: |
    spark.jars                              /opt/spark/jars/openmetadata-spark-agent.jar,/opt/spark/jars/mysql-connector-j-8.0.33.jar
    spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener
    spark.openmetadata.transport.type       openMetadata
    spark.openmetadata.transport.hostPort   http://openmetadata-service:8585/api
    spark.openmetadata.transport.pipelineServiceName k8s_spark_service
---
apiVersion: batch/v1
kind: Job
metadata:
  name: spark-etl-with-lineage
spec:
  template:
    spec:
      containers:
      - name: spark-driver
        image: bitnami/spark:3.5.0
        env:
        - name: OPENMETADATA_JWT_TOKEN
          valueFrom:
            secretKeyRef:
              name: openmetadata-secret
              key: jwt-token
        volumeMounts:
        - name: spark-config
          mountPath: /opt/bitnami/spark/conf/spark-defaults.conf
          subPath: spark-defaults.conf
      volumes:
      - name: spark-config
        configMap:
          name: spark-lineage-config
      restartPolicy: Never
```

### ğŸ  Pour Standalone Cluster

```bash
# Configuration Standalone
export SPARK_MASTER_URL="spark://master-node:7077"
export SPARK_HOME="/opt/spark"

# Lancement avec lineage
$SPARK_HOME/bin/spark-submit \
  --master $SPARK_MASTER_URL \
  --total-executor-cores 8 \
  --executor-memory 4g \
  --jars /opt/spark-lineage/jars/openmetadata-spark-agent.jar,/opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=standalone_spark_service" \
  ton_job.py
```

### ğŸ’» Pour DÃ©veloppement Local

```bash
# Mode local avec lineage pour debug
spark-submit \
  --master "local[*]" \
  --driver-memory 2g \
  --jars ./jars/openmetadata-spark-agent.jar,./jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://localhost:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=dev_spark_service" \
  --conf "spark.openmetadata.transport.debugFacet=true" \
  mon_job_dev.py
```

---


## DÃ©pannage et diagnostic


### ProblÃ¨mes frÃ©quents et solutions

```mermaid
flowchart TD
    Start[â“ ProblÃ¨me Lineage] --> Check1{JARs prÃ©sents ?}
    
    Check1 -->|âŒ| Fix1[ğŸ“¦ TÃ©lÃ©charge les JARs]
    Check1 -->|âœ…| Check2{Token valide ?}
    
    Check2 -->|âŒ| Fix2[ğŸ”‘ RÃ©gÃ©nÃ¨re token OM]
    Check2 -->|âœ…| Check3{OM accessible ?}
    
    Check3 -->|âŒ| Fix3[ğŸŒ Check connectivitÃ© rÃ©seau]
    Check3 -->|âœ…| Check4{Config Spark OK ?}
    
    Check4 -->|âŒ| Fix4[âš™ï¸ VÃ©rifie spark.extraListeners]
    Check4 -->|âœ…| Check5{Logs d'erreur ?}
    
    Check5 -->|âœ…| Fix5[ğŸ› Mode debug activÃ©]
    Check5 -->|âŒ| Success[âœ… MystÃ¨re rÃ©solu !]
    
    Fix1 --> Test[ğŸ§ª Test Ã  nouveau]
    Fix2 --> Test
    Fix3 --> Test
    Fix4 --> Test
    Fix5 --> Test
```

#### ğŸ”¥ ClassNotFoundException : OpenLineageSparkListener

**Erreur :**
```
Exception in thread "main" java.lang.ClassNotFoundException: io.openlineage.spark.agent.OpenLineageSparkListener
```

**Solutions :**
```bash
# 1. VÃ©rifier que le JAR est prÃ©sent
ls -la /path/to/openmetadata-spark-agent.jar

# 2. VÃ©rifier que le JAR est dans le classpath
spark-submit --jars /path/to/openmetadata-spark-agent.jar --conf "spark.driver.extraClassPath=/path/to/openmetadata-spark-agent.jar" ...

# 3. Re-tÃ©lÃ©charger le JAR si corrompu
wget https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar
```

#### ğŸ« Erreur Token JWT

**Erreur :**
```
HTTP 401: Unauthorized - Token verification failed
```

**Solutions :**
```bash
# 1. VÃ©rifier que le token est valide
curl -H "Authorization: Bearer $OPENMETADATA_JWT_TOKEN" http://openmetadata:8585/api/v1/system/version

# 2. RÃ©gÃ©nÃ©rer un nouveau token
# Via OpenMetadata UI : Settings â†’ Bots â†’ ingestion-bot â†’ Generate New Token

# 3. VÃ©rifier les variables d'environnement
echo "Token: ${OPENMETADATA_JWT_TOKEN:0:20}..."
```

#### ğŸŒ ProblÃ¨mes de ConnectivitÃ©

**Erreur :**
```
Connection refused to http://openmetadata:8585/api
```

**Solutions :**
```bash
# 1. Test de connectivitÃ©
telnet openmetadata-host 8585

# 2. VÃ©rifier l'URL dans la config
# Attention : /api Ã  la fin est obligatoire !
spark.openmetadata.transport.hostPort=http://openmetadata:8585/api

# 3. RÃ©solution DNS (si en Docker)
nslookup openmetadata
```

#### âš™ï¸ Config Spark Non Prise en Compte

**Si le lineage ne marche pas :**

```bash
# 1. VÃ©rifier que le listener est bien configurÃ©
spark-submit --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" ...

# 2. VÃ©rifier les logs Spark
tail -f $SPARK_HOME/logs/spark-*.log | grep -i openlineage

# 3. Activer le mode debug
--conf "spark.openmetadata.transport.debugFacet=true"
```

### ğŸ› Mode Debug Complet

```bash
# Lancement avec debug maximal
spark-submit \
  --master local[*] \
  --jars ./jars/openmetadata-spark-agent.jar,./jars/mysql-connector-j-8.0.33.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://localhost:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=debug_service" \
  --conf "spark.openmetadata.transport.debugFacet=true" \
  --conf "spark.sql.execution.arrow.pyspark.enabled=false" \
  --driver-java-options "-Dlog4j.configuration=file:log4j.properties" \
  ton_job.py 2>&1 | tee debug_lineage.log

# Analyser les logs
grep -i "openlineage\|openmetadata" debug_lineage.log
```


### Checklist de diagnostic

1. **âœ… JARs** : `ls -la *.jar` 
2. **âœ… Token** : `curl -H "Authorization: Bearer $TOKEN" http://om:8585/api/v1/system/version`
3. **âœ… RÃ©seau** : `telnet openmetadata-host 8585`
4. **âœ… Config** : VÃ©rifier `spark.extraListeners`
5. **âœ… Services OM** : Pipeline service crÃ©Ã© dans OpenMetadata
6. **âœ… Logs** : Chercher "openlineage" dans les logs Spark

---


## RÃ©sultats attendus dans OpenMetadata

### ğŸ¯ Ce que tu vas voir

```mermaid
graph TB
    subgraph OM[ğŸ“‹ OpenMetadata Interface]
        
        subgraph Services[ğŸ¢ Services]
            PS[âš¡ Pipeline Service: Spark]
            DBS[ğŸ—„ï¸ Database Service: MySQL Source]
            DBT[ğŸ—„ï¸ Database Service: MySQL Target]
        end
        
        subgraph Pipelines[ğŸ”„ Pipelines]
            P1[ğŸ“ mon_etl_pipeline]
            P2[ğŸ“Š customer_analytics_pipeline] 
            P3[ğŸ”„ daily_processing_pipeline]
        end
        
        subgraph Lineage[ğŸ”— Data Lineage]
            L1[ğŸ“Š Table customers] --> L2[ğŸ”„ Spark Transform]
            L3[ğŸ“Š Table orders] --> L2
            L2 --> L4[ğŸ“ˆ Table customer_analytics]
        end
        
        subgraph Metadata[ğŸ“‹ MÃ©tadonnÃ©es]
            M1[ğŸ“Š SchÃ©mas]
            M2[ğŸ·ï¸ Colonnes] 
            M3[ğŸ” Types de donnÃ©es]
            M4[â±ï¸ Historique exÃ©cutions]
        end
    end
    
    Services --> Pipelines
    Pipelines --> Lineage
    Lineage --> Metadata
```


### Lineage Graph interactif

Dans OpenMetadata, vous visualiserez :
- Un graphe de lineage entre tables sources, transformations Spark et tables cibles
- La traÃ§abilitÃ© colonne Ã  colonne
- L'historique d'exÃ©cution des pipelines
- Les mÃ©triques d'exÃ©cution (lignes traitÃ©es, durÃ©e, statut)
- Les tags automatiques (Spark, ETL, nom du pipeline)

---

## ğŸ‡¬ğŸ‡§ English

### ğŸ“‹ Quick Start Guide

This project provides automatic data lineage tracking for Apache Spark jobs with OpenMetadata integration. Perfect for production environments where you need to track ETL pipelines automatically.

### âš¡ Key Features
- âœ… **Zero-code lineage** - Just add JARs and configuration
- âœ… **Real-time tracking** - Captures lineage as jobs execute
- âœ… **Multi-platform** - Works on YARN, Kubernetes, Standalone
- âœ… **Column-level lineage** - Tracks transformations at column level
- âœ… **Production ready** - Used in enterprise environments

### ï¿½ Quick Integration

**For existing Spark environments:**

```bash
# 1. Download required JARs
wget https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar

# 2. Add to your spark-submit
spark-submit \
  --jars openmetadata-spark-agent.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://your-openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_JWT_TOKEN" \
  your_existing_job.py
```

**For Docker testing:**

```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
docker-compose up -d
./run-example.sh
```

### ğŸ“Š Access Points
- **OpenMetadata UI**: http://localhost:8585 (admin/admin)
- **Spark Master UI**: http://localhost:8080
- **Example Databases**: MySQL on ports 3307/3308

---

## ğŸ‡ªğŸ‡¸ EspaÃ±ol

### ğŸ“‹ DescripciÃ³n
Este proyecto implementa un sistema de linaje automÃ¡tico para Apache Spark con OpenMetadata. Rastrea automÃ¡ticamente los flujos de datos entre tablas MySQL a travÃ©s de transformaciones ETL de Spark.

### âš¡ CaracterÃ­sticas
- âœ… **Linaje automÃ¡tico** - Captura automÃ¡tica de relaciones entre tablas
- âœ… **Multi-tabla** - Soporte para transformaciones complejas (7 tablas: 4 fuentes + 3 objetivos)
- âœ… **OpenMetadata** - IntegraciÃ³n nativa con OpenMetadata 1.9.7
- âœ… **Docker** - Entorno completo con Docker Compose
- âœ… **Spark 3.5.0** - Ãšltima versiÃ³n estable de Apache Spark

### ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
```

2. **Iniciar entorno Docker**
```bash
docker-compose up -d
```

3. **Ejecutar trabajo Spark**
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf "spark.driver.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  --conf "spark.executor.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### ğŸ“Š Servicios
- **OpenMetadata UI**: http://localhost:8585
- **Spark Master UI**: http://localhost:8080
- **MySQL Fuente**: localhost:3308 (root/password)
- **MySQL Objetivo**: localhost:3307 (root/password)

---

## ğŸ‡µğŸ‡¹ PortuguÃªs

### ğŸ“‹ DescriÃ§Ã£o
Este projeto implementa um sistema de linhagem automÃ¡tica para Apache Spark com OpenMetadata. Rastreia automaticamente os fluxos de dados entre tabelas MySQL atravÃ©s de transformaÃ§Ãµes ETL do Spark.

### âš¡ Funcionalidades
- âœ… **Linhagem automÃ¡tica** - Captura automÃ¡tica de relacionamentos entre tabelas
- âœ… **Multi-tabela** - Suporte para transformaÃ§Ãµes complexas (7 tabelas: 4 fontes + 3 destinos)
- âœ… **OpenMetadata** - IntegraÃ§Ã£o nativa com OpenMetadata 1.9.7
- âœ… **Docker** - Ambiente completo com Docker Compose
- âœ… **Spark 3.5.0** - Ãšltima versÃ£o estÃ¡vel do Apache Spark

### ğŸ› ï¸ InstalaÃ§Ã£o

1. **Clonar o repositÃ³rio**
```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
```

2. **Iniciar ambiente Docker**
```bash
docker-compose up -d
```

3. **Executar job Spark**
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf "spark.driver.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  --conf "spark.executor.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### ğŸ“Š ServiÃ§os
- **OpenMetadata UI**: http://localhost:8585
- **Spark Master UI**: http://localhost:8080
- **MySQL Fonte**: localhost:3308 (root/password)
- **MySQL Destino**: localhost:3307 (root/password)

---

## ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

### ğŸ“‹ Ø§Ù„ÙˆØµÙ
ÙŠØ·Ø¨Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù†Ø¸Ø§Ù… Ù†Ø³Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù€ Apache Spark Ù…Ø¹ OpenMetadata. ÙŠØªØªØ¨Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ØªØ¯ÙÙ‚Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ÙŠÙ† Ø¬Ø¯Ø§ÙˆÙ„ MySQL Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­ÙˆÙŠÙ„Ø§Øª Spark ETL.

### âš¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
- âœ… **Ù†Ø³Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠ** - Ø§Ù„ØªÙ‚Ø§Ø· ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
- âœ… **Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„** - Ø¯Ø¹Ù… Ù„Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© (7 Ø¬Ø¯Ø§ÙˆÙ„: 4 Ù…ØµØ§Ø¯Ø± + 3 Ø£Ù‡Ø¯Ø§Ù)
- âœ… **OpenMetadata** - Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ù…Ø¹ OpenMetadata 1.9.7
- âœ… **Docker** - Ø¨ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ø¹ Docker Compose
- âœ… **Spark 3.5.0** - Ø£Ø­Ø¯Ø« Ø¥ØµØ¯Ø§Ø± Ù…Ø³ØªÙ‚Ø± Ù…Ù† Apache Spark

### ğŸ› ï¸ Ø§Ù„ØªØ«Ø¨ÙŠØª

**1. Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹**
```bash
git clone https://github.com/Monsau/sparklineage.git
cd sparklineage
```

**2. Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Docker**
```bash
docker-compose up -d
```

**3. ØªØ´ØºÙŠÙ„ Ù…Ù‡Ù…Ø© Spark**
```bash
docker exec spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf "spark.driver.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  --conf "spark.executor.extraClassPath=/opt/bitnami/spark/jars/openmetadata-spark-agent.jar:/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar" \
  /opt/bitnami/spark/complex_spark_lineage_job.py
```

### ğŸ“Š Ø§Ù„Ø®Ø¯Ù…Ø§Øª
- **OpenMetadata UI**: http://localhost:8585
- **Spark Master UI**: http://localhost:8080
- **MySQL Ø§Ù„Ù…ØµØ¯Ø±**: localhost:3308 (root/password)
- **MySQL Ø§Ù„Ù‡Ø¯Ù**: localhost:3307 (root/password)

---

## ğŸ—ï¸ Architecture | Arquitectura | Arquitetura | Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL Source  â”‚â”€â”€â”€â”€â”‚   Apache Spark  â”‚â”€â”€â”€â”€â”‚  MySQL Target   â”‚
â”‚   (Port 3308)   â”‚    â”‚   ETL Process   â”‚    â”‚   (Port 3307)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  OpenMetadata   â”‚
                    â”‚   (Port 8585)   â”‚
                    â”‚ Lineage Trackingâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure | Estructura | Estrutura | Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

```
sparklineage/
â”œâ”€â”€ complex_spark_lineage_job.py    # Main Spark ETL job
â”œâ”€â”€ docker-compose.yml              # Docker services configuration
â”œâ”€â”€ samples/                        # SQL initialization examples
â”‚   â”œâ”€â”€ init-source.sql
â”‚   â”œâ”€â”€ init-target.sql
â”‚   â””â”€â”€ create_*.sql
â”œâ”€â”€ jars/                           # Required JAR files
â”‚   â”œâ”€â”€ openmetadata-spark-agent.jar
â”‚   â””â”€â”€ mysql-connector-j-8.0.33.jar
â””â”€â”€ README.md                       # This file
```

## ğŸ¤ Contributing | Contribuir | Contribuindo | Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©

1. Fork the project | Haz fork del proyecto | FaÃ§a fork do projeto | Ù‚Ù… Ø¨Ø¹Ù…Ù„ fork Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
2. Create your feature branch | Crea tu rama de caracterÃ­sticas | Crie sua branch de feature | Ø£Ù†Ø´Ø¦ ÙØ±Ø¹ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
3. Commit your changes | Haz commit de tus cambios | FaÃ§a commit das suas mudanÃ§as | Ù‚Ù… Ø¨Ø¹Ù…Ù„ commit Ù„ØªØºÙŠÙŠØ±Ø§ØªÙƒ
4. Push to the branch | Haz push a la rama | FaÃ§a push para a branch | Ø§Ø¯ÙØ¹ Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ø¹
5. Open a Pull Request | Abre un Pull Request | Abra um Pull Request | Ø§ÙØªØ­ Pull Request

## ğŸ“ License | Licencia | LicenÃ§a | Ø§Ù„ØªØ±Ø®ÙŠØµ

This project is licensed under the MIT License - see the LICENSE file for details.

---

