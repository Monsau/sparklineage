# üîß Guide d'Int√©gration Spark ‚Üí OpenMetadata

*Les configurations concr√®tes √† ajouter dans tes projets Spark existants*

## üéØ Ce Guide pour Qui ?

```mermaid
flowchart LR
    Dev[üë®‚Äçüíª Dev Spark] --> Question{Tu as d√©j√† un projet Spark ?}
    Question -->|‚úÖ Oui| Integration[üìã Ce guide est pour toi !]
    Question -->|‚ùå Non| Demo[üê≥ Use le docker-compose d'abord]
    
    Integration --> Add[‚ûï Ajouter quelques configs]
    Add --> Magic[‚ú® Lineage automatique !]
    
    Demo --> Learn[üìö Apprendre avec l'exemple]
    Learn --> Integration
```

**Tu es dans un de ces cas ?**
- ‚úÖ Tu as d√©j√† un cluster Spark (YARN, K8s, Standalone)
- ‚úÖ Tu as des jobs ETL en production 
- ‚úÖ Tu veux tracer le lineage sans refaire tes jobs
- ‚úÖ Tu veux int√©grer OpenMetadata dans ton infra existante

**Alors c'est parti !**

---

## üì¶ √âtape 1 : R√©cup√©rer les JARs (une fois pour toutes)

### üîó T√©l√©chargement Automatique

```bash
#!/bin/bash
# setup-spark-lineage.sh - Script √† lancer une fois

echo "üöÄ Installation des JARs pour Spark + OpenMetadata"

# Dossier pour les JARs (ajuste selon ton env)
JARS_DIR="/opt/spark-lineage/jars"
mkdir -p $JARS_DIR
cd $JARS_DIR

echo "üì¶ T√©l√©chargement OpenMetadata Spark Agent..."
wget -O openmetadata-spark-agent.jar \
  https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar

echo "üì¶ T√©l√©chargement MySQL Connector..."
wget -O mysql-connector-j-8.0.33.jar \
  https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.33/mysql-connector-java-8.0.33.jar

echo "üì¶ T√©l√©chargement PostgreSQL Connector (optionnel)..."
wget -O postgresql-42.7.1.jar \
  https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.1/postgresql-42.7.1.jar

# V√©rification
echo "‚úÖ JARs t√©l√©charg√©s :"
ls -la *.jar

# Permissions
chmod 644 *.jar

echo "üéâ Setup termin√© ! JARs dans : $JARS_DIR"
```

### üìã URLs de T√©l√©chargement Direct

| JAR | URL | Taille |
|-----|-----|--------|
| **OpenMetadata Agent** | `https://github.com/open-metadata/OpenMetadata/releases/download/1.9.7/openmetadata-spark-agent.jar` | ~50MB |
| **MySQL Connector** | `https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.33/mysql-connector-java-8.0.33.jar` | ~2MB |
| **PostgreSQL Connector** | `https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.1/postgresql-42.7.1.jar` | ~1MB |

---

## üîë √âtape 2 : Token JWT OpenMetadata

### üöÄ M√©thode Rapide (via UI)

```mermaid
sequenceDiagram
    participant Toi as üë®‚Äçüíª Toi
    participant Browser as üåê Navigateur  
    participant OM as üìä OpenMetadata
    
    Toi->>Browser: http://openmetadata:8585
    Browser->>OM: Connexion (admin/admin)
    Toi->>OM: Settings ‚Üí Bots
    OM->>Toi: Liste des bots disponibles
    Toi->>OM: Clique "ingestion-bot"
    OM->>Toi: D√©tails du bot
    Toi->>OM: "Generate New Token"
    OM->>Toi: üé´ JWT Token g√©n√©r√© !
    Note over Toi: Copie et sauvegarde le token
```

### ü§ñ M√©thode API (pour les scripts)

```bash
#!/bin/bash
# get-openmetadata-token.sh

OPENMETADATA_URL="http://your-openmetadata:8585"
ADMIN_USER="admin"  
ADMIN_PASS="admin"

echo "üîë G√©n√©ration du token JWT OpenMetadata..."

# Login et r√©cup√©ration du token
TOKEN_RESPONSE=$(curl -s -X POST "${OPENMETADATA_URL}/api/v1/users/login" \
  -H "Content-Type: application/json" \
  -d "{
    \"email\": \"${ADMIN_USER}\",
    \"password\": \"${ADMIN_PASS}\"
  }")

# Extraction du token d'acc√®s
ACCESS_TOKEN=$(echo $TOKEN_RESPONSE | jq -r '.accessToken')

if [ "$ACCESS_TOKEN" != "null" ]; then
    echo "‚úÖ Token obtenu avec succ√®s !"
    echo "üîë Token JWT : $ACCESS_TOKEN"
    
    # Sauvegarde dans un fichier
    echo "export OPENMETADATA_JWT_TOKEN='$ACCESS_TOKEN'" > openmetadata-token.env
    echo "üíæ Token sauv√© dans openmetadata-token.env"
    
    # Test du token
    curl -s -H "Authorization: Bearer $ACCESS_TOKEN" \
         "${OPENMETADATA_URL}/api/v1/system/version" > /dev/null
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ Token valid√© avec succ√®s !"
    else
        echo "‚ùå Erreur de validation du token"
    fi
else
    echo "‚ùå Impossible d'obtenir le token"
    echo "Response: $TOKEN_RESPONSE"
fi
```

---

## ‚öôÔ∏è √âtape 3 : Configurations par Type d'Environnement

### üè¢ YARN Cluster (Production)

#### üìÑ M√©thode 1 : spark-defaults.conf Global

```properties
# /opt/spark/conf/spark-defaults.conf
# Configuration globale pour tous les jobs Spark

# === JARs OpenMetadata ===
spark.jars                              hdfs://namenode:9000/spark-lineage/openmetadata-spark-agent.jar,hdfs://namenode:9000/spark-lineage/mysql-connector-j-8.0.33.jar

# === Lineage Core ===
spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener

# === OpenMetadata Transport ===
spark.openmetadata.transport.type       openMetadata
spark.openmetadata.transport.hostPort   http://openmetadata-prod.company.com:8585/api
spark.openmetadata.transport.pipelineServiceName production_spark_cluster

# === Options Performance ===
spark.openmetadata.transport.timeout    30
spark.openmetadata.transport.includeInputs   true
spark.openmetadata.transport.includeOutputs  true

# === Security (token via variable d'env) ===
# spark.openmetadata.transport.jwtToken sera d√©fini via SPARK_CONF_DIR/spark-env.sh
```

#### üîß spark-env.sh (pour le token s√©curis√©)

```bash
# /opt/spark/conf/spark-env.sh
# Variables d'environnement s√©curis√©es

# Token OpenMetadata (r√©cup√©r√© depuis un vault ou fichier s√©curis√©)
if [ -f "/etc/spark/secrets/openmetadata-token" ]; then
    OPENMETADATA_JWT_TOKEN=$(cat /etc/spark/secrets/openmetadata-token)
    export SPARK_CONF="$SPARK_CONF --conf spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN"
fi

# Autres configs s√©curis√©es
export SPARK_LOCAL_DIRS="/tmp/spark"
export SPARK_WORKER_DIR="/var/log/spark"
```

#### üöÄ Lancement Job YARN

```bash
#!/bin/bash
# run-yarn-job-with-lineage.sh

# Variables sp√©cifiques au job
JOB_NAME="customer_analytics_etl"
JOB_DATE=$(date +%Y%m%d_%H%M%S)
PIPELINE_NAME="${JOB_NAME}_${JOB_DATE}"

# Token (depuis un vault s√©curis√© en prod)
source /etc/spark/secrets/openmetadata.env

echo "üöÄ Lancement job YARN avec lineage : $JOB_NAME"

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --name "$PIPELINE_NAME" \
  --driver-memory 4g \
  --driver-cores 2 \
  --executor-memory 8g \
  --executor-cores 3 \
  --num-executors 20 \
  --queue production \
  --conf "spark.openmetadata.transport.pipelineName=$PIPELINE_NAME" \
  --conf "spark.openmetadata.transport.pipelineDescription=ETL $JOB_NAME - $(date)" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  hdfs://namenode:9000/spark-jobs/${JOB_NAME}.py

echo "‚úÖ Job lanc√© ! Pipeline: $PIPELINE_NAME"
echo "üìä Lineage disponible dans OpenMetadata"
```

### üê≥ Kubernetes (Cloud Native)

#### üìÑ ConfigMap pour la Configuration

```yaml
# spark-lineage-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-lineage-config
  namespace: spark-jobs
data:
  spark-defaults.conf: |
    # Configuration Spark + OpenMetadata pour K8s
    spark.jars                              /opt/spark/jars/openmetadata-spark-agent.jar,/opt/spark/jars/mysql-connector-j-8.0.33.jar
    spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener
    spark.openmetadata.transport.type       openMetadata
    spark.openmetadata.transport.hostPort   http://openmetadata-service.openmetadata.svc.cluster.local:8585/api
    spark.openmetadata.transport.pipelineServiceName k8s_spark_cluster
    spark.openmetadata.transport.timeout    30
    
  log4j.properties: |
    # Logs sp√©cifiques OpenMetadata
    log4j.logger.io.openlineage=INFO
    log4j.logger.org.openmetadata=INFO
    log4j.rootLogger=WARN, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
```

#### üîë Secret pour le Token

```yaml
# openmetadata-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: openmetadata-secret
  namespace: spark-jobs
type: Opaque
data:
  # Token JWT encod√© en base64 (echo -n "ton_token" | base64)
  jwt-token: ZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKU1V6STFOaUo5...
```

#### üöÄ SparkApplication CRD

```yaml
# spark-etl-job.yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: customer-etl-with-lineage
  namespace: spark-jobs
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "bitnami/spark:3.5.0"
  imagePullPolicy: Always
  
  mainApplicationFile: "hdfs://namenode:9000/spark-jobs/customer_etl.py"
  
  sparkConf:
    "spark.openmetadata.transport.pipelineName": "k8s_customer_etl_$(date +%Y%m%d_%H%M%S)"
    "spark.openmetadata.transport.pipelineDescription": "Customer ETL Pipeline - Kubernetes"
    
  driver:
    cores: 2
    coreLimit: "2000m"
    memory: "4g"
    memoryOverhead: "1g"
    serviceAccount: spark-driver
    
    env:
    - name: OPENMETADATA_JWT_TOKEN
      valueFrom:
        secretKeyRef:
          name: openmetadata-secret
          key: jwt-token
    
    envVars:
      SPARK_CONF_DIR: "/opt/spark/conf"
    
    volumeMounts:
    - name: spark-config
      mountPath: /opt/spark/conf
      
  executor:
    cores: 3
    instances: 10
    memory: "8g"
    memoryOverhead: "2g"
    
    env:
    - name: OPENMETADATA_JWT_TOKEN
      valueFrom:
        secretKeyRef:
          name: openmetadata-secret
          key: jwt-token
    
    volumeMounts:
    - name: spark-config
      mountPath: /opt/spark/conf
      
  volumes:
  - name: spark-config
    configMap:
      name: spark-lineage-config
      
  restartPolicy:
    type: Never
```

#### üîß D√©ploiement K8s

```bash
#!/bin/bash
# deploy-k8s-spark-lineage.sh

echo "üöÄ D√©ploiement Spark + OpenMetadata sur K8s"

# 1. Cr√©er le namespace
kubectl create namespace spark-jobs

# 2. Appliquer les configurations
kubectl apply -f spark-lineage-configmap.yaml
kubectl apply -f openmetadata-secret.yaml

# 3. Lancer le job Spark
kubectl apply -f spark-etl-job.yaml

# 4. Suivre les logs
echo "üìä Suivi des logs :"
kubectl logs -f spark-customer-etl-with-lineage-driver -n spark-jobs

# 5. V√©rifier le statut
kubectl get sparkapplications -n spark-jobs
```

### üè† Standalone Cluster (On-Premise)

#### üìÑ Configuration Master/Workers

```bash
# /opt/spark/conf/spark-defaults.conf (sur master et workers)

# === JARs Distribution ===
spark.jars                              file:///opt/spark-lineage/jars/openmetadata-spark-agent.jar,file:///opt/spark-lineage/jars/mysql-connector-j-8.0.33.jar

# === Lineage ===
spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener

# === OpenMetadata ===
spark.openmetadata.transport.type       openMetadata
spark.openmetadata.transport.hostPort   http://openmetadata.internal:8585/api
spark.openmetadata.transport.pipelineServiceName standalone_spark_cluster

# === Performance Standalone ===
spark.serializer                        org.apache.spark.serializer.KryoSerializer
spark.sql.adaptive.enabled              true
spark.sql.adaptive.coalescePartitions.enabled true
```

#### üöÄ Script de Lancement Standalone

```bash
#!/bin/bash
# run-standalone-job.sh

# Configuration environnement
export SPARK_HOME="/opt/spark"
export JAVA_HOME="/opt/jdk-11"
export SPARK_MASTER_URL="spark://spark-master.internal:7077"

# Informations job
JOB_NAME="$1"
JOB_FILE="$2"

if [ -z "$JOB_NAME" ] || [ -z "$JOB_FILE" ]; then
    echo "Usage: $0 <job_name> <job_file.py>"
    exit 1
fi

# Token OpenMetadata (depuis fichier s√©curis√©)
source /opt/spark-lineage/config/openmetadata.env

# Variables dynamiques
PIPELINE_NAME="${JOB_NAME}_$(date +%Y%m%d_%H%M%S)"
USER_NAME=$(whoami)

echo "üöÄ Lancement job Standalone : $JOB_NAME"
echo "üìä Pipeline : $PIPELINE_NAME"
echo "üîó Master : $SPARK_MASTER_URL"

$SPARK_HOME/bin/spark-submit \
  --master $SPARK_MASTER_URL \
  --total-executor-cores 16 \
  --executor-memory 6g \
  --driver-memory 2g \
  --conf "spark.openmetadata.transport.pipelineName=$PIPELINE_NAME" \
  --conf "spark.openmetadata.transport.pipelineDescription=Standalone job $JOB_NAME by $USER_NAME" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  $JOB_FILE

exit_code=$?

if [ $exit_code -eq 0 ]; then
    echo "‚úÖ Job termin√© avec succ√®s !"
    echo "üìä Lineage disponible : http://openmetadata.internal:8585"
else
    echo "‚ùå Job √©chou√© (code: $exit_code)"
fi

exit $exit_code
```

### üíª D√©veloppement Local

#### üîß Configuration IDE (IntelliJ/VSCode)

```properties
# spark-local.conf (pour d√©veloppement)
spark.master                            local[*]
spark.jars                              ./jars/openmetadata-spark-agent.jar,./jars/mysql-connector-j-8.0.33.jar
spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener

# === Dev OpenMetadata ===
spark.openmetadata.transport.type       openMetadata
spark.openmetadata.transport.hostPort   http://localhost:8585/api
spark.openmetadata.transport.pipelineServiceName dev_spark
spark.openmetadata.transport.debugFacet true

# === Performance Local ===
spark.driver.memory                     2g
spark.executor.memory                   2g
spark.sql.shuffle.partitions            4
spark.default.parallelism               4
```

#### üêç Template de D√©veloppement Python

```python
#!/usr/bin/env python3
"""
Template de d√©veloppement Spark avec lineage OpenMetadata
Usage: python dev-spark-job.py
"""

import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def create_dev_spark_session():
    """
    Session Spark pour d√©veloppement avec lineage
    """
    
    # Configuration d√©veloppement
    app_name = f"DEV-{os.path.basename(__file__)}"
    developer = os.getenv('USER', 'developer')
    
    # Token depuis variable d'env ou fichier local
    jwt_token = os.getenv('OPENMETADATA_JWT_TOKEN')
    if not jwt_token and os.path.exists('.openmetadata-token'):
        with open('.openmetadata-token', 'r') as f:
            jwt_token = f.read().strip()
    
    if not jwt_token:
        print("‚ö†Ô∏è  OPENMETADATA_JWT_TOKEN non d√©fini, lineage d√©sactiv√©")
        return SparkSession.builder.appName(app_name).getOrCreate()
    
    # Chemins JARs (relatifs pour le dev)
    jars_path = "./jars"
    if not os.path.exists(jars_path):
        print("‚ö†Ô∏è  Dossier ./jars non trouv√©, lineage d√©sactiv√©") 
        return SparkSession.builder.appName(app_name).getOrCreate()
    
    openmetadata_jar = f"{jars_path}/openmetadata-spark-agent.jar"
    mysql_jar = f"{jars_path}/mysql-connector-j-8.0.33.jar"
    
    print(f"üîß Session Spark DEV : {app_name}")
    print(f"üë®‚Äçüíª D√©veloppeur : {developer}")
    print(f"üìä Lineage : Activ√©")
    
    return SparkSession.builder \
        .appName(app_name) \
        .master("local[*]") \
        .config("spark.jars", f"{openmetadata_jar},{mysql_jar}") \
        .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener") \
        .config("spark.openmetadata.transport.type", "openMetadata") \
        .config("spark.openmetadata.transport.hostPort", "http://localhost:8585/api") \
        .config("spark.openmetadata.transport.jwtToken", jwt_token) \
        .config("spark.openmetadata.transport.pipelineServiceName", "dev_spark") \
        .config("spark.openmetadata.transport.pipelineName", f"dev_{developer}_{app_name}") \
        .config("spark.openmetadata.transport.debugFacet", "true") \
        .getOrCreate()

def main():
    """Job de d√©veloppement avec lineage"""
    
    spark = create_dev_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        print("üöÄ D√©but job de d√©veloppement...")
        
        # === Ton code de test ici ===
        
        # Exemple : lecture CSV
        df = spark.read.option("header", "true").csv("./data/sample.csv")
        
        # Transformation simple
        df_processed = df.select("id", "name").filter(col("id").isNotNull())
        
        # Sauvegarde
        df_processed.write.mode("overwrite").parquet("./output/processed_data")
        
        print(f"‚úÖ Job termin√© ! {df_processed.count()} lignes trait√©es")
        print("üìä Check le lineage dans OpenMetadata : http://localhost:8585")
        
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
```

---

## üéØ √âtape 4 : Configuration OpenMetadata (Services)

### üè¢ Cr√©er les Services de Donn√©es

```mermaid
sequenceDiagram
    participant Dev as üë®‚Äçüíª D√©veloppeur
    participant OM as üìä OpenMetadata UI
    participant Spark as ‚ö° Spark Job
    
    Dev->>OM: 1. Cr√©er Database Services
    Note over OM: MySQL Source, PostgreSQL, etc.
    
    Dev->>OM: 2. Cr√©er Pipeline Service
    Note over OM: Spark Cluster Service
    
    Dev->>OM: 3. Tester connexions
    OM->>Dev: ‚úÖ Services valid√©s
    
    Dev->>Spark: 4. Lancer job avec lineage
    Spark->>OM: 5. Envoyer m√©tadonn√©es
    OM->>Dev: 6. Lineage disponible !
```

#### üìä Database Services (Sources et Destinations)

Dans OpenMetadata UI : **Settings** ‚Üí **Services** ‚Üí **Databases** ‚Üí **Add Database Service**

**Pour MySQL :**
```json
{
  "name": "mysql_production_source",
  "serviceType": "MySQL",
  "connection": {
    "config": {
      "type": "Mysql",
      "scheme": "mysql+pymysql",
      "username": "spark_reader",
      "password": "***",
      "hostPort": "mysql-prod.company.com:3306",
      "database": "ecommerce"
    }
  }
}
```

**Pour PostgreSQL :**
```json
{
  "name": "postgresql_analytics_target", 
  "serviceType": "Postgres",
  "connection": {
    "config": {
      "type": "Postgres",
      "scheme": "postgresql+psycopg2",
      "username": "spark_writer",
      "password": "***",
      "hostPort": "postgres-analytics.company.com:5432",
      "database": "analytics"
    }
  }
}
```

#### ‚ö° Pipeline Service (Spark Cluster)

**Settings** ‚Üí **Services** ‚Üí **Pipelines** ‚Üí **Add Pipeline Service**

```json
{
  "name": "production_spark_cluster",
  "serviceType": "CustomPipeline", 
  "connection": {
    "config": {
      "type": "CustomPipeline",
      "sourcePythonClass": "metadata.ingestion.source.pipeline.pipeline_service.PipelineServiceSource"
    }
  }
}
```

### üîß Script d'Automatisation Services

```bash
#!/bin/bash
# setup-openmetadata-services.sh

OPENMETADATA_URL="http://openmetadata.company.com:8585"
JWT_TOKEN="$OPENMETADATA_JWT_TOKEN"

echo "üè¢ Cr√©ation des services OpenMetadata..."

# Function pour cr√©er un service
create_service() {
    local service_name="$1"
    local service_config="$2"
    
    echo "üìä Cr√©ation service : $service_name"
    
    curl -s -X POST "${OPENMETADATA_URL}/api/v1/services/databaseServices" \
      -H "Authorization: Bearer ${JWT_TOKEN}" \
      -H "Content-Type: application/json" \
      -d "$service_config"
}

# Service MySQL Production
mysql_config='{
  "name": "mysql_production",
  "serviceType": "MySQL",
  "connection": {
    "config": {
      "type": "Mysql",
      "scheme": "mysql+pymysql",
      "username": "'"$MYSQL_USER"'",
      "password": "'"$MYSQL_PASS"'",
      "hostPort": "'"$MYSQL_HOST"':3306",
      "database": "production"
    }
  }
}'

# Service Spark Cluster
spark_config='{
  "name": "production_spark_cluster",
  "serviceType": "CustomPipeline",
  "connection": {
    "config": {
      "type": "CustomPipeline"
    }
  }
}'

# Cr√©ation des services
create_service "mysql_production" "$mysql_config"
create_service "spark_cluster" "$spark_config"

echo "‚úÖ Services cr√©√©s avec succ√®s !"
```

---

## üîç √âtape 5 : Test et Validation

### üß™ Script de Test Complet

```bash
#!/bin/bash
# test-spark-lineage.sh - Validation compl√®te de l'int√©gration

echo "üß™ Test d'int√©gration Spark + OpenMetadata"

# === Variables ===
SPARK_HOME="/opt/spark"
JARS_DIR="/opt/spark-lineage/jars"
OPENMETADATA_URL="http://openmetadata.company.com:8585"

# === V√©rifications pr√©alables ===
echo "üîç V√©rifications pr√©alables..."

# 1. JARs pr√©sents
if [ ! -f "$JARS_DIR/openmetadata-spark-agent.jar" ]; then
    echo "‚ùå JAR OpenMetadata manquant"
    exit 1
fi
echo "‚úÖ JAR OpenMetadata trouv√©"

# 2. Spark disponible
if [ ! -f "$SPARK_HOME/bin/spark-submit" ]; then
    echo "‚ùå Spark non trouv√© dans $SPARK_HOME"
    exit 1
fi
echo "‚úÖ Spark trouv√©"

# 3. OpenMetadata accessible
if ! curl -s "$OPENMETADATA_URL/api/v1/system/version" > /dev/null; then
    echo "‚ùå OpenMetadata non accessible"
    exit 1
fi
echo "‚úÖ OpenMetadata accessible"

# 4. Token JWT valide
if [ -z "$OPENMETADATA_JWT_TOKEN" ]; then
    echo "‚ùå Token JWT non d√©fini"
    exit 1
fi

if ! curl -s -H "Authorization: Bearer $OPENMETADATA_JWT_TOKEN" \
    "$OPENMETADATA_URL/api/v1/system/version" > /dev/null; then
    echo "‚ùå Token JWT invalide"
    exit 1
fi
echo "‚úÖ Token JWT valid√©"

# === Job de test ===
echo "üöÄ Lancement job de test..."

# Cr√©ation d'un job Python simple
TEST_JOB="/tmp/test-lineage-job.py"
cat > $TEST_JOB << 'EOF'
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("LineageTest").getOrCreate()

# Donn√©es de test
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])

# Transformation simple
df_adults = df.filter(col("age") >= 18).withColumn("category", lit("adult"))

# Affichage (pas de destination pour ce test)
df_adults.show()

print("‚úÖ Test lineage termin√© avec succ√®s !")
spark.stop()
EOF

# Lancement avec lineage
$SPARK_HOME/bin/spark-submit \
  --master local[2] \
  --jars "$JARS_DIR/openmetadata-spark-agent.jar" \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=$OPENMETADATA_URL/api" \
  --conf "spark.openmetadata.transport.jwtToken=$OPENMETADATA_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=test_spark_service" \
  --conf "spark.openmetadata.transport.pipelineName=lineage_integration_test" \
  $TEST_JOB

# V√©rification dans OpenMetadata
echo "üîç V√©rification du lineage dans OpenMetadata..."
sleep 5

# Check si le pipeline existe
PIPELINE_CHECK=$(curl -s -H "Authorization: Bearer $OPENMETADATA_JWT_TOKEN" \
  "$OPENMETADATA_URL/api/v1/services/pipelineServices/test_spark_service/pipelines" | \
  jq -r '.data[] | select(.name=="lineage_integration_test") | .name')

if [ "$PIPELINE_CHECK" = "lineage_integration_test" ]; then
    echo "‚úÖ Pipeline cr√©√© dans OpenMetadata !"
    echo "üìä Lineage visible sur : $OPENMETADATA_URL"
else
    echo "‚ö†Ô∏è  Pipeline pas encore visible (peut prendre quelques minutes)"
fi

# Nettoyage
rm -f $TEST_JOB

echo "üéâ Test d'int√©gration termin√© !"
```

### üìä Validation du Lineage

```python
#!/usr/bin/env python3
"""
validate-lineage.py - V√©rification du lineage via API OpenMetadata
"""

import requests
import json
import os
import sys

class OpenMetadataValidator:
    def __init__(self, base_url, jwt_token):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'Authorization': f'Bearer {jwt_token}',
            'Content-Type': 'application/json'
        }
    
    def check_pipeline_service(self, service_name):
        """V√©rifier qu'un service pipeline existe"""
        url = f"{self.base_url}/api/v1/services/pipelineServices/{service_name}"
        response = requests.get(url, headers=self.headers)
        return response.status_code == 200
    
    def get_pipelines(self, service_name):
        """R√©cup√©rer les pipelines d'un service"""
        url = f"{self.base_url}/api/v1/services/pipelineServices/{service_name}/pipelines"
        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            return response.json().get('data', [])
        return []
    
    def get_pipeline_lineage(self, pipeline_fqn):
        """R√©cup√©rer le lineage d'un pipeline"""
        url = f"{self.base_url}/api/v1/lineage/table/{pipeline_fqn}"
        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            return response.json()
        return None
    
    def validate_integration(self, service_name, pipeline_name=None):
        """Validation compl√®te de l'int√©gration"""
        print(f"üîç Validation service : {service_name}")
        
        # 1. V√©rifier le service
        if not self.check_pipeline_service(service_name):
            print(f"‚ùå Service {service_name} introuvable")
            return False
        print(f"‚úÖ Service {service_name} trouv√©")
        
        # 2. Lister les pipelines
        pipelines = self.get_pipelines(service_name)
        print(f"üìä {len(pipelines)} pipeline(s) trouv√©(s)")
        
        if not pipelines:
            print("‚ö†Ô∏è  Aucun pipeline avec lineage d√©tect√©")
            return True  # Service OK mais pas de pipeline encore
        
        # 3. V√©rifier lineage des pipelines
        for pipeline in pipelines:
            name = pipeline.get('name')
            fqn = pipeline.get('fullyQualifiedName')
            
            if pipeline_name and name != pipeline_name:
                continue
                
            print(f"üîó V√©rification lineage : {name}")
            
            lineage = self.get_pipeline_lineage(fqn)
            if lineage:
                nodes = lineage.get('nodes', [])
                edges = lineage.get('downstreamEdges', [])
                print(f"  üìä {len(nodes)} entit√©s, {len(edges)} relations")
            else:
                print(f"  ‚ö†Ô∏è  Pas de lineage pour {name}")
        
        return True

def main():
    # Configuration
    openmetadata_url = os.getenv('OPENMETADATA_URL', 'http://localhost:8585')
    jwt_token = os.getenv('OPENMETADATA_JWT_TOKEN')
    
    if not jwt_token:
        print("‚ùå OPENMETADATA_JWT_TOKEN requis")
        sys.exit(1)
    
    # Services √† v√©rifier
    services_to_check = [
        'production_spark_cluster',
        'dev_spark',
        'k8s_spark_cluster',
        'test_spark_service'
    ]
    
    validator = OpenMetadataValidator(openmetadata_url, jwt_token)
    
    print("üß™ Validation de l'int√©gration Spark + OpenMetadata")
    print(f"üîó URL : {openmetadata_url}")
    
    all_good = True
    for service in services_to_check:
        if not validator.validate_integration(service):
            all_good = False
        print()
    
    if all_good:
        print("‚úÖ Int√©gration valid√©e avec succ√®s !")
    else:
        print("‚ùå Probl√®mes d√©tect√©s dans l'int√©gration")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## üéØ R√©capitulatif des Configurations √† Ajouter

### üìã Checklist d'Int√©gration

| √âtape | Configuration | Statut |
|-------|---------------|--------|
| **1. JARs** | ‚úÖ openmetadata-spark-agent.jar | ‚¨ú |
| | ‚úÖ mysql-connector-j-8.0.33.jar | ‚¨ú |
| **2. Token** | ‚úÖ JWT Token OpenMetadata | ‚¨ú |
| **3. Spark Config** | ‚úÖ spark.extraListeners | ‚¨ú |
| | ‚úÖ spark.openmetadata.transport.* | ‚¨ú |
| **4. Services OM** | ‚úÖ Database Services (sources) | ‚¨ú |
| | ‚úÖ Pipeline Service (Spark) | ‚¨ú |
| **5. Test** | ‚úÖ Job de test avec lineage | ‚¨ú |

### üöÄ Configuration Minimale (pour commencer)

```bash
# La config minimale pour voir le lineage
spark-submit \
  --jars /path/to/openmetadata-spark-agent.jar \
  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \
  --conf "spark.openmetadata.transport.type=openMetadata" \
  --conf "spark.openmetadata.transport.hostPort=http://your-openmetadata:8585/api" \
  --conf "spark.openmetadata.transport.jwtToken=YOUR_JWT_TOKEN" \
  --conf "spark.openmetadata.transport.pipelineServiceName=your_service" \
  ton_job.py
```

### üèÜ Configuration Compl√®te (pour la prod)

```properties
# spark-defaults.conf complet
spark.jars                              /opt/spark-lineage/jars/openmetadata-spark-agent.jar
spark.extraListeners                    io.openlineage.spark.agent.OpenLineageSparkListener
spark.openmetadata.transport.type       openMetadata
spark.openmetadata.transport.hostPort   http://openmetadata.company.com:8585/api
spark.openmetadata.transport.pipelineServiceName production_spark_cluster
spark.openmetadata.transport.timeout    30
spark.openmetadata.transport.includeInputs   true
spark.openmetadata.transport.includeOutputs  true
spark.openmetadata.transport.debugFacet false
```

---

## üéâ Et Voil√† !

Une fois ces configurations ajout√©es, tous tes jobs Spark existants vont automatiquement envoyer leur lineage vers OpenMetadata. **Z√©ro modification de code n√©cessaire !**

**Ce que tu vas voir dans OpenMetadata :**
- üîó **Graphiques de lineage** interactifs
- üìä **M√©tadonn√©es enrichies** (sch√©mas, colonnes, types)
- ‚è±Ô∏è **Historique des ex√©cutions** avec timestamps
- üè∑Ô∏è **Tags automatiques** (Spark, Pipeline names, etc.)
- üëÅÔ∏è **Tra√ßabilit√© colonne par colonne**

**Questions ?** Check la section troubleshooting du README principal !

*Made with ‚ù§Ô∏è pour les √©quipes qui veulent du lineage sans se prendre la t√™te*